"""Publish the datasette to fly.io.

We use custom logic here because the datasette-publish-fly plugin bakes the
uncompressed databases into the image, which makes the image too large.

We compress the databases before baking them into the image. Then we decompress
them at runtime to a Fly volume mounted at /data. This avoids a long download
at startup, and allows us stay within the Fly.io 8GB image size limit.

The volume handling is done manually outside of this publish.py script - it
should be terraformed at some point.

Some static fly.io deployment-related files live in ./fly:
* fly.toml - service configuration
* run.sh - service entrypoint

Apart from that: the Dockerfile and dataset-specific
metadata.yml/inspect-data.json are generated by this script.
"""

import json
import logging
import secrets
import sys
from pathlib import Path
from subprocess import check_call, check_output  # noqa: S404

import click

from pudl.helpers import check_tables_have_metadata
from pudl.metadata.classes import DatasetteMetadata
from pudl.workspace.setup import PudlPaths

logging.basicConfig(format="%(asctime)s %(message)s", level=logging.INFO)

DOCKERFILE_TEMPLATE = """
FROM python:3.11.0-slim-bullseye

ENV DATABASES '{databases}'
ENV DATASETTE_PORT 8081
ENV NGINX_PORT 8080

RUN apt-get update
RUN apt-get install -y zstd nginx
RUN pip install -U datasette datasette-cluster-map datasette-vega datasette-block-robots

# set up nginx + enable real IP module
COPY nginx.conf /usr/share/nginx/nginx.conf
COPY 50-mod-http-realip.conf /etc/nginx/modules-enabled/

# the two symlinks allow nginx logs to get written out to stdout/stderr
RUN mkdir /data \
    && ln -sf /dev/stdout /var/log/nginx/access.log \
    && ln -sf /dev/stderr /var/log/nginx/error.log

COPY . /app
WORKDIR /app

EXPOSE ${{NGINX_PORT}}
ENV DATASETTE_SECRET '{datasette_secret}'
CMD ["./run.sh"]
"""


def make_dockerfile(databases: list[str]) -> str:
    """Write a dockerfile from template, to use in fly deploy.

    We write this from template so we can generate a datasette secret. This way
    we don't have to manage secrets at all.
    """
    datasette_secret = secrets.token_hex(16)
    return DOCKERFILE_TEMPLATE.format(
        datasette_secret=datasette_secret,
        databases=" ".join(f"/data/{d}" for d in databases),
    )


def inspect_data(datasets: list[str], pudl_output: Path) -> str:
    """Pre-inspect databases to generate some metadata for Datasette.

    This is done in the image build process in datasette-publish-fly, but since
    we don't have access to the databases in the build process we have to
    inspect before building the Docker image.
    """
    inspect_output = json.loads(
        check_output(  # noqa: S603
            [
                "datasette",
                "inspect",
            ]
            + [str(pudl_output / ds) for ds in datasets]
        )
    )

    for dataset in inspect_output:
        name = Path(inspect_output[dataset]["file"]).name
        new_filepath = Path("/data") / name
        inspect_output[dataset]["file"] = str(new_filepath)
    return inspect_output


@click.command(context_settings={"help_option_names": ["-h", "--help"]})
@click.option(
    "--production",
    "deploy",
    flag_value="production",
    help="Deploy Datasette to fly.io PRODUCTION environment",
)
@click.option(
    "--staging",
    "deploy",
    flag_value="staging",
    help="Deploy Datasette to fly.io STAGING environment",
)
@click.option(
    "--local",
    "-l",
    "deploy",
    flag_value="local",
    help="Deploy Datasette locally for testing or debugging purposes. Note that"
    "you have to stop the docker instance manually to terminate this server.",
)
@click.option(
    "--metadata",
    "-m",
    "deploy",
    flag_value="metadata",
    help="Generate the Datasette metadata.yml in current directory, but do not deploy.",
)
@click.option(
    "--only",
    "only_databases",
    required=False,
    multiple=True,
    default=(),
    help="Only deploy specific SQLite files - can greatly speed up iteration while developing.",
)
@click.argument(
    "fly_args",
    required=False,
    nargs=-1,
)
def deploy_datasette(
    deploy: str, fly_args: tuple[str], only_databases: tuple[str]
) -> int:
    """Generate deployment files and deploy Datasette either locally or to fly.io.

    Any additional arguments after -- will be passed through to flyctl if deploying to
    fly.io. E.g. the following would build ouputs for fly.io, but not actually deploy:

    python publish.py --fly -- --build-only
    """
    logging.info(f"Deploying to {deploy.upper()}...")
    pudl_output = PudlPaths().pudl_output

    all_databases = (
        ["pudl.sqlite"]
        + sorted(str(p.name) for p in pudl_output.glob("ferc*.sqlite"))
        + ["censusdp1tract.sqlite"]
    )
    databases = list(only_databases if only_databases else all_databases)

    fly_dir = Path(__file__).parent.absolute() / "fly"
    docker_path = fly_dir / "Dockerfile"
    inspect_path = fly_dir / "inspect-data.json"
    metadata_path = fly_dir / "metadata.yml"
    metadata_yml = DatasetteMetadata.from_data_source_ids(pudl_output).to_yaml()

    logging.info(f"Writing Datasette metadata to: {metadata_path}")
    with metadata_path.open("w") as f:
        f.write(metadata_yml)
    check_tables_have_metadata(metadata_yml, databases)

    if deploy == "metadata":
        logging.info("Only writing metadata. Aborting now.")
        return 0

    logging.info(f"Inspecting DBs for datasette: {databases}...")
    inspect_output = inspect_data(databases, pudl_output)
    with inspect_path.open("w") as f:
        f.write(json.dumps(inspect_output))

    logging.info("Writing Dockerfile...")
    with docker_path.open("w") as f:
        f.write(make_dockerfile(databases))

    logging.info(f"Compressing {databases} and putting into docker context...")
    check_call(  # noqa: S603
        ["tar", "-a", "-czvf", fly_dir / "all_dbs.tar.zst"] + databases,
        cwd=pudl_output,
    )

    # OK, now we have a Dockerfile + the right context. Time to run the dang
    # container somehwere.
    if deploy in {"production", "staging"}:
        fly_dir = Path(__file__).parent.absolute() / "fly"
        logging.info(f"Deploying {deploy} to fly.io...")
        docker_path = fly_dir / "Dockerfile"
        inspect_path = fly_dir / "inspect-data.json"
        metadata_path = fly_dir / "metadata.yml"

        logging.info(f"Inspecting DBs for datasette: {databases}...")
        inspect_output = inspect_data(databases, pudl_output)
        with inspect_path.open("w") as f:
            f.write(json.dumps(inspect_output))

        logging.info(f"Writing Datasette metadata to: {metadata_path}")
        with metadata_path.open("w") as f:
            f.write(metadata_yml)

        logging.info("Writing Dockerfile...")
        with docker_path.open("w") as f:
            f.write(make_dockerfile(databases))

        logging.info(f"Compressing {databases} and putting into docker context...")
        check_call(  # noqa: S603
            ["tar", "-a", "-czvf", fly_dir / "all_dbs.tar.zst"] + databases,
            cwd=pudl_output,
        )

        logging.info("Running fly deploy...")
        cmd = ["/usr/bin/env", "flyctl", "deploy", "-c", fly_dir / f"{deploy}.toml"]
        if fly_args:
            cmd = cmd + list(fly_args)
        check_call(cmd, cwd=fly_dir)  # noqa: S603
        logging.info("Deploy finished!")

    elif deploy == "local":
        logging.info("Running Datasette locally...")
        check_call(  # noqa: S603
            ["/usr/bin/env", "docker", "build", "-t", "pudl_datasette:local", "."],
            cwd=fly_dir,
        )
        check_call(  # noqa: S603
            ["/usr/bin/env", "docker", "run", "-p", "8080:8080", "pudl_datasette:local"]
        )

    else:
        logging.error(f"Unrecognized deployment destination: {deploy=}")
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(deploy_datasette())
