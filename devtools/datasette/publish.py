"""Publish the datasette to fly.io.

We use custom logic here because the datasette-publish-fly plugin bakes the
uncompressed databases into the image, which makes the image too large.

We compress the databases before baking them into the image. Then we decompress
them at runtime to a Fly volume mounted at /data. This avoids a long download
at startup, and allows us stay within the Fly.io 8GB image size limit.

The volume handling is done manually outside of this publish.py script - it
should be terraformed at some point.

Some static fly.io deployment-related files live in ./fly:
* fly.toml - service configuration
* run.sh - service entrypoint

Apart from that: the Dockerfile and dataset-specific
metadata.yml/inspect-data.json are generated by this script.
"""

import json
import logging
import secrets
import sys
from pathlib import Path
from subprocess import check_call, check_output  # noqa: S404

import click

from pudl.helpers import check_tables_have_metadata, create_datasette_metadata_yaml
from pudl.workspace.setup import PudlPaths

logging.basicConfig(format="%(asctime)s %(message)s", level=logging.INFO)

DOCKERFILE_TEMPLATE = """
FROM python:3.11.0-slim-bullseye
COPY . /app
WORKDIR /app

RUN apt-get update
RUN apt-get install -y zstd

ENV DATASETTE_SECRET '{datasette_secret}'
RUN pip install -U datasette datasette-cluster-map datasette-vega datasette-block-robots
ENV PORT 8080
EXPOSE 8080

CMD ["./run.sh"]
"""


def make_dockerfile() -> str:
    """Write a dockerfile from template, to use in fly deploy.

    We write this from template so we can generate a datasette secret. This way
    we don't have to manage secrets at all.
    """
    datasette_secret = secrets.token_hex(16)
    return DOCKERFILE_TEMPLATE.format(datasette_secret=datasette_secret)


def inspect_data(datasets: list[str], pudl_output: Path) -> str:
    """Pre-inspect databases to generate some metadata for Datasette.

    This is done in the image build process in datasette-publish-fly, but since
    we don't have access to the databases in the build process we have to
    inspect before building the Docker image.
    """
    inspect_output = json.loads(
        check_output(
            [  # noqa: S603
                "datasette",
                "inspect",
            ]
            + [str(pudl_output / ds) for ds in datasets]
        )
    )

    for dataset in inspect_output:
        name = Path(inspect_output[dataset]["file"]).name
        new_filepath = Path("/data") / name
        inspect_output[dataset]["file"] = str(new_filepath)
    return inspect_output


@click.command(context_settings={"help_option_names": ["-h", "--help"]})
@click.option(
    "--fly",
    "-f",
    "deploy",
    flag_value="fly",
    help="Deploy Datasette to fly.io.",
    default=True,
)
@click.option(
    "--local",
    "-l",
    "deploy",
    flag_value="local",
    help="Deploy Datasette locally for testing or debugging purposes.",
)
@click.option(
    "--metadata",
    "-m",
    "deploy",
    flag_value="metadata",
    help="Generate the Datasette metadata.yml in current directory, but do not deploy.",
)
@click.argument(
    "fly_args",
    required=False,
    nargs=-1,
)
def deploy_datasette(deploy: str, fly_args: tuple[str]) -> int:
    """Generate deployment files and deploy Datasette either locally or to fly.io.

    Any additional arguments after -- will be passed through to flyctl if deploying to
    fly.io. E.g. the following would build ouputs for fly.io, but not actually deploy:

    python publish.py --fly -- --build-only
    """
    pudl_output = PudlPaths().pudl_output
    metadata_yml = create_datasette_metadata_yaml()

    databases = (
        ["pudl.sqlite"]
        + sorted(str(p.name) for p in pudl_output.glob("ferc*.sqlite"))
        + ["censusdp1tract.sqlite"]
    )
    # Make sure we have the expected metadata for databases
    # headed to deployment.
    if deploy != "metadata":
        check_tables_have_metadata(metadata_yml, databases)
    if deploy == "fly":
        logging.info("Deploying to fly.io...")
        fly_dir = Path(__file__).parent.absolute() / "fly"
        docker_path = fly_dir / "Dockerfile"
        inspect_path = fly_dir / "inspect-data.json"
        metadata_path = fly_dir / "metadata.yml"

        logging.info(f"Inspecting DBs for datasette: {databases}...")
        inspect_output = inspect_data(databases, pudl_output)
        with inspect_path.open("w") as f:
            f.write(json.dumps(inspect_output))

        logging.info(f"Writing Datasette metadata to: {metadata_path}")
        with metadata_path.open("w") as f:
            f.write(metadata_yml)

        logging.info("Writing Dockerfile...")
        with docker_path.open("w") as f:
            f.write(make_dockerfile())

        logging.info(f"Compressing {databases} and putting into docker context...")
        check_call(
            ["tar", "-a", "-czvf", fly_dir / "all_dbs.tar.zst"] + databases,  # noqa: S603
            cwd=pudl_output,
        )

        logging.info("Running fly deploy...")
        cmd = ["/usr/bin/env", "flyctl", "deploy"]
        if fly_args:
            cmd = cmd + list(fly_args)
        check_call(cmd, cwd=fly_dir)  # noqa: S603
        logging.info("Deploy finished!")

    elif deploy == "local":
        logging.info("Running Datasette locally...")
        metadata_path = pudl_output / "metadata.yml"
        logging.info(f"Writing Datasette metadata to: {metadata_path}")
        with metadata_path.open("w") as f:
            f.write(metadata_yml)

        check_call(
            ["/usr/bin/env", "datasette", "serve", "-m", "metadata.yml"] + databases,  # noqa: S603
            cwd=pudl_output,
        )

    elif deploy == "metadata":
        metadata_path = Path.cwd() / "metadata.yml"
        logging.info(f"Writing Datasette metadata to: {metadata_path}")
        with metadata_path.open("w") as f:
            f.write(metadata_yml)

    else:
        logging.error(f"Unrecognized deployment destination: {deploy=}")
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(deploy_datasette())
