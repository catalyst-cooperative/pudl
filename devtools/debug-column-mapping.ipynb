{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing column mapping for Excel spreadsheets\n",
    "This notebook is designed to quickly test column maps for Excel spreadsheets. It will flag the following:\n",
    "1) Column names that are input but don't exist in the actual data\n",
    "2) Column names present in the raw data but not mapped\n",
    "3) Invalid inputs for pages and files in `page_map.csv` and `file_map.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, select the raw dataset you're going to be mapping and locate all relevant file directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from zipfile import ZipFile\n",
    "import sys\n",
    "import types\n",
    "\n",
    "import pudl\n",
    "from pudl.workspace.datastore import ZenodoDoiSettings\n",
    "from pudl.extract.phmsagas import Extractor\n",
    "\n",
    "logger = pudl.logging_helpers.get_logger(\"__name__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"phmsagas\"\n",
    "doi_path = getattr(ZenodoDoiSettings(), dataset).replace(\"/\", \"-\")\n",
    "pudl_paths = pudl.workspace.setup.PudlPaths()\n",
    "data_path = os.path.join(pudl_paths.pudl_input,dataset,doi_path) # Get path to raw data\n",
    "map_path = os.path.join(Path(pudl.package_data.__file__).parents[0], dataset) # Get path to mapping CSVs\n",
    "ds = pudl.workspace.datastore.Datastore(pudl_paths.pudl_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## File Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, validate the file map. Make sure all file names included in the CSV actually exist in the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_map = pd.read_csv(\n",
    "            os.path.join(map_path, \"file_map.csv\"), index_col=0, comment=\"#\"\n",
    "        )\n",
    "raw_files = os.listdir(data_path)\n",
    "\n",
    "# For each file, if zipfile get list of file names contained inside\n",
    "all_files = []\n",
    "for file in raw_files:\n",
    "    if file.endswith(\"zip\"):\n",
    "        file_path = os.path.join(data_path, file)\n",
    "        file_list = ZipFile(file_path).namelist()\n",
    "        all_files.append({file_path: file_list})\n",
    "\n",
    "for table_files in file_map.values.tolist(): # For each table with a list of files\n",
    "    for file in table_files: # For each file included in this table\n",
    "        if file not in str(all_files): # Search the list of files for the file text, flag if not.\n",
    "            logger.warning(f\"File '{file}' not found in actual raw data. Check file name.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, read in the column mapping CSVs. For each one, read in the raw data and make sure no columns are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_name = pd.read_csv(\n",
    "            os.path.join(map_path, \"page_map.csv\"), index_col=0, comment=\"#\"\n",
    "        )\n",
    "skip_rows = pd.read_csv(\n",
    "            os.path.join(map_path, \"skiprows.csv\"), index_col=0, comment=\"#\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Map Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we don't care about missing raw columns, or we only want to check a particular table. Set parameters here to fine tune what you're checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_check = True # If false, only check that mapped columns are found in the raw dataset.\n",
    "                  # Useful when a table is split between several pages.\n",
    "table_subset = [] # Leave list empty to check all tables\n",
    "years_subset = [] # Use empty list if you want to check all years, otherwise supply a list of integers or a range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_zip(file: str, dicts: list[dict[str,str]]) -> str:\n",
    "    for dic in dicts:\n",
    "        match = [i for i in dic if file in dic[i]]\n",
    "        if match == []:\n",
    "            continue\n",
    "        return match[0]\n",
    "\n",
    "\n",
    "for page in file_map.index:\n",
    "    if not table_subset or page in table_subset:\n",
    "        column_maps = pd.read_csv(\n",
    "                os.path.join(map_path, \"column_maps\", f\"{page}.csv\"), index_col=0, comment=\"#\"\n",
    "            )\n",
    "        for index in file_map.columns: \n",
    "            if not years_subset or int(index) in years_subset:\n",
    "                file = file_map.loc[page,index] # Get file name\n",
    "                if file == \"-1\":\n",
    "                    logger.info(f\"No data for year {index}\")\n",
    "                else:\n",
    "                    logger.info(f\"Checking column maps for {page}, {index}\")\n",
    "                    archive = ZipFile(find_zip(file, all_files)) # Open zipfile and read file\n",
    "                    with archive.open(file) as excel_file:\n",
    "                        raw_file = pd.read_excel(\n",
    "                                    excel_file,\n",
    "                                    sheet_name=sheet_name.loc[page,index],\n",
    "                                    skiprows=skip_rows.loc[page,index],\n",
    "                                )\n",
    "                    raw_file = pudl.helpers.simplify_columns(raw_file) # Add pre-processing step used before column rename\n",
    "                    raw_columns = raw_file.columns # Get raw column names\n",
    "                    mapped_columns = column_maps.loc[:, index].dropna()\n",
    "                    raw_missing = [col for col in raw_columns if col not in mapped_columns.values]\n",
    "                    mapped_missing = [col for col in mapped_columns if col not in raw_columns.values]\n",
    "                    if raw_missing and raw_check:\n",
    "                        logger.warning(f\"Raw columns {raw_missing} from {file} are not mapped.\")\n",
    "                    if mapped_missing:\n",
    "                        logger.warning(f\"Mapped columns {mapped_missing} do not exist in the raw data file {file}\")\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back and fix any incorrectly labelled columns. Then run the cell above again, until all columns are correctly labelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractor Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETTINGS FOR EXTRACTOR\n",
    "extractor_phmsagas = Extractor(ds=ds)\n",
    "\n",
    "# recommend changing the loglevel here to warning to only get the baddies\n",
    "pudl.logging_helpers.configure_root_logger(loglevel=\"WARNING\")\n",
    "\n",
    "# IF you wanna restrict the years\n",
    "working_years = list(range(1990,2023))\n",
    "# IF you want to restrict the pages to extract here is a lil way to do that\n",
    "# you give pages_you_want_to_extract a lil of pages you want to extract\n",
    "# if pages_you_want_to_extract if nothing, you'll get the standard pages\n",
    "pages_you_want_to_extract = []\n",
    "all_pages = extractor_phmsagas._metadata.get_all_pages()\n",
    "def _new_page_getter(self):\n",
    "    if pages_you_want_to_extract:\n",
    "        return pages_you_want_to_extract\n",
    "    else:\n",
    "        return all_pages\n",
    "extractor_phmsagas._metadata.get_all_pages = types.MethodType(_new_page_getter, extractor_phmsagas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THE EXTRACTOR\n",
    "extracted_dfs = extractor_phmsagas.extract(year=working_years)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
