{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with EPA CEMS data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CEMS or <a href='https://www.epa.gov/emc/emc-continuous-emission-monitoring-systems'>**Continuous Emissions Monitoring Systems**</a> are used to track power plant's compliance with EPA emission standards. Included the data are hourly measurements of gross load, SO2, CO2, and NOx emissions associated with a given point source. The EPA's <a href='https://www.epa.gov/airmarkets'>Clean Air Markets Division</a> has collected CEMS data stretching back to 1995 and publicized it in their <a href='https://campd.epa.gov/'>data portal</a>. Combinging the CEMS data with geospatial, EIA and FERC data can enable greater and more specific analysis of utilities and their generation facilities. This notebook provides examples of working with the CEMS data in pudl."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE**: This Notebook presuposes access to the parquet files where the full CEMS data are stored.\n",
    "\n",
    "#### Notebook Contents:\n",
    "* **<a href='#setup'>Setup</a>**\n",
    "* **<a href='#access'>Accessing CEMS data</a>**\n",
    " - <a href='#1subset'>1. Select a subset of raw data using Dask</a>\n",
    " - <a href='#2transfer'>2. Transfer desired data to pandas</a>\n",
    " - <a href='#3pickle'>3. Store custom CEMS dataframe as a pickle</a>\n",
    "* **<a href='#manipulating'>Manipulating & Visualizing CEMS data</a>**\n",
    " - <a href='#emap'>1. Simple Choropleth</a>\n",
    " - <a href='#pcem'>2. Proportional Coordinates Map</a>\n",
    " - <a href='#glc'>3. State-to-State Gross Load Comparison</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## Setup\n",
    "\n",
    "The following kernels enable interaction with the CEMS dataset through pudl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# 3rd party libraries\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sqlalchemy as sa\n",
    "\n",
    "# Local libraries\n",
    "import pudl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable viewing of logging outputs\n",
    "logger=logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(stream=sys.stdout)\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to pudl database\n",
    "from pudl.workspace.setup import PudlPaths\n",
    "\n",
    "pudl_engine = sa.create_engine(PudlPaths().pudl_db)\n",
    "pudl_out = pudl.output.pudltabl.PudlTabl(pudl_engine, freq='AS') #annual frequency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following settings and variables may be changed to impact the processing of this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display settings\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CEMS dates\n",
    "CEMS_year = 2018\n",
    "CEMS_year_range = range(2010, 2012)\n",
    "\n",
    "# State selection\n",
    "state_subset = ['CO', 'TX', 'WY', 'MN', 'OH', 'PA', 'WV', 'FL', 'GE', 'CA']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='access'></a>\n",
    "## Accessing CEMS data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CEMS dataset is enormous! It contains hourly emissions data on an hourly basis between 1995 and 2020, meaning that the full dataset is close to a billion rows and takes up 100 GB of space. That's a lot to process when you may only need a fraction of it for analysis. The following steps will help you access and work with CEMS efficiently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1subset'></a>\n",
    "### 1. Select a subset of raw data using Dask\n",
    "\n",
    "Dask is a python package that parallelizes pandas dataframes so that you can access larger-than-memory data. With Dask, you can select the subset of CEMS data that you'd like to analyse *before* loading the data into a dataframe. While in Dask, you can interact with the data as if it were in a pandas dataframe. \n",
    "\n",
    "We'll start with a single year and offer an option to integrate a range of dates below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pudl.workspace.setup import PudlPaths\n",
    "# Locate the data for the given year/s on your hard drive.\n",
    "epacems_path = (PudlPaths().output_dir / f'/epacems/year={CEMS_year}')\n",
    "\n",
    "# Create a Dask object for preliminary data interaction\n",
    "cems_dd = dd.read_parquet(epacems_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a Dask dataframe you can learn things about the data such as column names and datatypes without having to load all of the data. If you take a look at the length of the Dask dataframe, you'll understand why we're not in pandas yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cems_dd) # This shows how many rows there are for one year!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the CEMS fields available below. The records are organized by the date and time of their measurement as well as the EIA plant id (`plant_id_eia`) and EPA unit id (`unitid`) they coorespond to. The EPA unit id is EPA's most granular level of emissions tracing; it represents a singular \"smokestack\" where emissions data are monitored and recorded. Depending on the unit in question, this unit may reflect a single generator (in the case of a combustion gas turbine where emissions are directly associated with generation), or a group of inter-operating biolers and generators (such as with steam powered generators where one or more boilers, possibly with differing fuel types, provide mechanical power to turbines). As a result, the EPA unit id *does not map directly onto EIA's generator id*, rather, it serves as its own unique grouping. \n",
    "\n",
    "The EPA is in the process of publishing a \"crosswalk\" spreadsheet that links EPA units to EIA's more granular boiler and generator ids. This information is forthcoming and will be integrated into pudl as soon as possible.\n",
    "\n",
    "For more information on the individual fields, refer to the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cems_dd.columns.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know what's available, pick the columns you'd like to work with, and aggregate rows as necessary. Note that the `state` and `measurement_code` columns are categorical datatypes, meaning that they will overwhelm your computer memory if included in the list of columns you'd like to `groupby`. In pandas, this is solved by including the statement `observed=True` in the `groupby`, but with Dask we'll solve this by changing the datatype to string. As mentioned previously, the dataset is very large. If the Dask dataframe you construct is too similar to the original dataset -- imagine the example below without the `groupby` -- the client will be unable to load it in pandas and the kernel will attempt to run indefinitely (or until it crashes your computer). The dataset below should load in a couple of minutes when transfered to pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of the columns you'd like to include in your analysis\n",
    "my_cols = [\n",
    "    'state',\n",
    "    'plant_id_eia', \n",
    "    'unitid',\n",
    "    'so2_mass_lbs',\n",
    "    'nox_mass_lbs',\n",
    "    'co2_mass_tons'\n",
    "]\n",
    "\n",
    "# Select emissions data are grouped by state, plant_id and unit_id\n",
    "# Remember to change the datatype for 'state' from category to string\n",
    "my_cems_dd = (\n",
    "    dd.read_parquet(epacems_path, columns=my_cols)\n",
    "    .assign(state=lambda x: x['state'].astype('string'))\n",
    "    .groupby(['plant_id_eia', 'unitid', 'state'])[\n",
    "        ['so2_mass_lbs', 'nox_mass_lbs', 'co2_mass_tons']]\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2transfer'></a>\n",
    "### 2. Transfer desired data to pandas\n",
    "\n",
    "Now that you've selected the data you want to work with, we'll transfer it to pandas so that all rows are accessible. It'll take a moment to run because there are so many rows. If it takes longer than a couple of minutes, check to see that your Dask dataset is altered enough from it's original form. Remember, it's in Dask because the data is bigger than your computer's memory! You'll have to do some grouping or paring down in order to access its entirety in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas dataframe out of your Dask dataframe and add a column to \n",
    "# indicate the year the data are coming from.\n",
    "client = Client()\n",
    "my_cems_df = (\n",
    "    client.compute(my_cems_dd)\n",
    "    .result()\n",
    "    .assign(year=CEMS_year)\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cems_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get data from *multiple* years, run the following code block. It's commented out because it takes a while to run and isn't required to run the full notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years = CEMS_years\n",
    "# multi_year_cems_df = pd.DataFrame()\n",
    "\n",
    "# for yr in years:\n",
    "#     epacems_path = (PudlPaths().output_dir + f'/epacems/year={yr}')\n",
    "#     cems_dd = (\n",
    "#         dd.read_parquet(epacems_path, columns=my_cols)\n",
    "#         .assign(state=lambda x: x['state'].astype('string'))\n",
    "#         .groupby(['plant_id_eia', 'unitid', 'state'])[\n",
    "#             ['so2_mass_lbs', 'nox_mass_lbs', 'co2_mass_tons']]\n",
    "#         .sum())\n",
    "#     cems_df = (\n",
    "#         client.compute(cems_dd)\n",
    "#         .result()\n",
    "#         .assign(year=yr))\n",
    "#     multi_year_cems_df = pd.concat([multi_year_cems_df, cems_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_year_cems_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3pickle'></a>\n",
    "### 3. Store custom CEMS dataframe as a pickle\n",
    "\n",
    "Because CEMS takes a while to run, it may be in your best interest to save your finalized CEMS dataframes as pickle files. This will prevent you from having to run the entire Dask-to-pandas process over and over if you restart the notebook and you want to access your carve-out of CEMS. Rather, it will save a local copy of your CEMS dataframe that it can access in a matter of seconds. Uncomment the following to set up a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Savings CEMS as a pickle file\n",
    "#path = os.getcwd()\n",
    "#my_cems_df.to_pickle(path + '/MY_CEMS_DF.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing CEMS as a pickle file\n",
    "#my_cems_df = pd.read_pickle(path + '/MY_CEMS_DF.pkl')\n",
    "#my_cems_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='manipulating'></a>\n",
    "## Manipulating & Visualizing CEMS data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have access to CEMS in pandas, lets see what we can do!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='emap'></a>\n",
    "### 1. Simple Choropleth\n",
    "##### *Visualizing CEMS data*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by mapping which states have the highest CO2 emissions from power plants in 2018. States with darker colors will indicate higher CO2 emissions. To do this, we'll need to merge a geodataframe of the US with the desired emissions data from each state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prep US geospatial data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pre-existing pudl shapefile for state outlines\n",
    "# FIXME(zane): this is outdated and references non-existing method get_census2010_dgf\n",
    "pudl_settings = None\n",
    "assert pudl_settings is not None\n",
    "us_map_df = (\n",
    "    pudl.analysis.service_territory.get_census2010_gdf(pudl_settings, 'state')\n",
    "    .rename({'STUSPS10': 'state'}, axis=1)\n",
    "    .to_crs(\"EPSG:3395\") # Change the projection\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prep CEMS data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lbs to tons for so2 and nox and remove old columns\n",
    "# Aggregate CEMS emissions data to the state level\n",
    "cems_map_df = (\n",
    "    my_cems_df.assign(\n",
    "        so2_mass_tons=lambda x: x.so2_mass_lbs * 0.0005,\n",
    "        nox_mass_tons=lambda x: x.nox_mass_lbs * 0.0005\n",
    "    ).drop(columns=['so2_mass_lbs', 'nox_mass_lbs', 'plant_id_eia'], axis=1)\n",
    "    .groupby(['state', 'year']).sum(min_count=1)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine with Geo-data and Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine CEMS and map dataframes\n",
    "states_cems_gdf = pd.merge(us_map_df, cems_map_df, on='state', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add plots for the US, HI, and AK\n",
    "\n",
    "# The column on which to base the choroplath\n",
    "choro_col = 'co2_mass_tons'\n",
    "\n",
    "us_fig, us_ax = plt.subplots(figsize=(15, 10))\n",
    "#ak_hi_fig, (ak_ax, hi_ax) = plt.subplots(ncols=2)\n",
    "\n",
    "states_cems_gdf.plot(column=choro_col, cmap='Reds', linewidth=0.8, edgecolor='black', ax=us_ax)\n",
    "#states_cems_gdf.plot(column=choro_col, cmap='Reds', linewidth=0.8, edgecolor='black', ax=ak_ax)\n",
    "#states_cems_gdf.plot(column=choro_col, cmap='Reds', linewidth=0.8, edgecolor='black', ax=hi_ax)\n",
    "\n",
    "us_ax.set_xlim(-1.45e7, -0.7e7) # Used to position US in center of the graph\n",
    "us_ax.set_ylim(0.25e7, 0.65e7)  # Used to position US in center of the graph\n",
    "us_ax.set_title('CO2 Emissions from Power Plants in 2018 (Megatons)', fontdict={'fontsize': '18'})\n",
    "us_ax.axis('off')  # Remove lat and long tick marks\n",
    "#ak_ax.set_xlim(1.9e7, 6.7e6)  #(-2e7, -1.4e7)\n",
    "#ak_ax.set_ylim(0.6e7, 1.2e7)\n",
    "#hi_ax.set_xlim(-1.71e7, -1.8e7)\n",
    "#hi_ax.set_ylim(2e6, 2.6e6)\n",
    "\n",
    "# Add a legend\n",
    "vmax = states_cems_gdf[f'{choro_col}'].max() / 1000000 # (convert from tons to megatons)\n",
    "sm = plt.cm.ScalarMappable(cmap='Reds', norm=plt.Normalize(vmin=0, vmax=vmax))\n",
    "sm._A = []\n",
    "cbar = us_fig.colorbar(sm, orientation=\"horizontal\", pad=0, aspect = 50, label='CO2 Emissions (MT)')\n",
    "\n",
    "from matplotlib import axes\n",
    "axes.Axes.mouseover\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pcem'></a>\n",
    "### 2. Proportional Coordinates Map\n",
    "##### *Integrate CEMS emission quantities with EIA plant location data*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to integreate CEMS with other datasets in pudl, you'll need to start by integrating CEMS with a dataset that also has a field for `plant_id_eia`. If you want to integrate with FERC later on, you'll also want a dataset that has a field for `plant_id_pudl`. Integrating CEMS with EIA860 data will provide coordinates for each plant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prep CEMS data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate CEMS data to the plant level, adjust units for visualization purposes\n",
    "cems_df = (\n",
    "    my_cems_df\n",
    "    .copy()\n",
    "    .assign(\n",
    "        co2_mass_mt=lambda df: df.co2_mass_tons / 10000 # measure in 10K tons\n",
    "    ).drop(columns=['co2_mass_tons'], axis=1)\n",
    "    .groupby(['plant_id_eia', 'state', 'year'])\n",
    "    .sum(min_count=1)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prep EIA data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab EIA 860 plant data that matched the year selected for CEMS\n",
    "plants_eia860 = (\n",
    "    pudl_out.plants_eia860()\n",
    "    .assign(year=lambda df: df.report_date.dt.year)\n",
    "    .query(\"year==@CEMS_year\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine EIA and CEMS data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine CEMS and EIA on plant_id_eia, state, and year\n",
    "eia860_cems_df = (\n",
    "    pd.merge(plants_eia860, cems_df, on=['plant_id_eia', 'state', 'year'], how='inner')\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overlay Coordinates on Base Map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lat and long data cols into plotable points in geopandas\n",
    "# Make CRS compatile with base map\n",
    "eia860_cems_gdf = (\n",
    "    gpd.GeoDataFrame(\n",
    "        eia860_cems_df, geometry=gpd.points_from_xy(\n",
    "            eia860_cems_df.longitude, eia860_cems_df.latitude))\n",
    "    .set_crs(epsg=4326, inplace=True) # necessary step before to_crs(epsg=3395)\n",
    "    .to_crs(epsg=3395)\n",
    ")\n",
    "\n",
    "# Make a base map\n",
    "us_fig, us_ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "base = us_map_df.plot(color='white', edgecolor='black', ax=us_ax)\n",
    "\n",
    "us_ax.set_xlim(-1.45e7, -0.7e7) # Used to position US in center of the graph\n",
    "us_ax.set_ylim(0.25e7, 0.65e7)  # Used to position US in center of the graph\n",
    "us_ax.set_title('CO2 Emissions from Power Plants in 2018 (10K tons)', fontdict={'fontsize': '20'})\n",
    "us_ax.axis('off')  # Remove lat and long tick marks\n",
    "\n",
    "# Plot the coordinates on top of the base map\n",
    "eia860_cems_df['alpha_co2'] = eia860_cems_df['co2_mass_mt'] \n",
    "eia860_cems_gdf.plot(ax=base, marker='o', color='red', markersize=eia860_cems_df['co2_mass_mt'], alpha=0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='glc'></a>\n",
    "### 3. State-to-State Gross Load and Emissions Comparison\n",
    "##### *Compare state load and emissions profiles*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all states use locally sourced electricity, however, looking at load profiles of plants in a given state can provide a glimpse of who is responsible for the greatest fossil peak load. This allocation can also be done by utility, but requires a table that maps plant or unit ownership percentage by utility."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prep CEMS data (you'll have to re-load from Dask for this new data arrangement):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2018\n",
    "\n",
    "# A list of the columns you'd like to include in your analysis\n",
    "my_cols = [\n",
    "    'state',\n",
    "    'plant_id_eia', \n",
    "    'unitid',\n",
    "    'operating_datetime_utc',\n",
    "    'co2_mass_tons',\n",
    "    'gross_load_mw',\n",
    "]\n",
    "\n",
    "my_cems_dd = (\n",
    "    dd.read_parquet(epacems_path, columns=my_cols)\n",
    "    .assign(\n",
    "        state=lambda x: x['state'].astype('string'),\n",
    "        month=lambda x: x['operating_datetime_utc'].dt.month)\n",
    "    .groupby(['state', 'month'])['gross_load_mw', 'co2_mass_tons'].sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Create a pandas dataframe out of your Dask dataframe and add a column to \n",
    "# indicate the year the data are coming from.\n",
    "client = Client()\n",
    "my_cems_gl = (\n",
    "    client.compute(my_cems_dd)\n",
    "    .result()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select state subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_piv = my_cems_gl.pivot(columns='state', index=['month'], values=['gross_load_mw'])\n",
    "gl_piv_subset = gl_piv.iloc[:, gl_piv.columns.get_level_values(1).isin(state_subset)].copy()\n",
    "\n",
    "co2_piv = my_cems_gl.pivot(columns='state', index=['month'], values=['co2_mass_tons'])\n",
    "co2_piv_subset = co2_piv.iloc[:, co2_piv.columns.get_level_values(1).isin(state_subset)].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot load and emissions comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (gl_ax, co2_ax) = plt.subplots(1,2)\n",
    "\n",
    "gl_piv_subset.plot(\n",
    "    figsize=(15,8),\n",
    "    xticks=gl_piv_subset.index,\n",
    "    ylabel='Gross Load MW',\n",
    "    xlabel='Months',\n",
    "    ax=gl_ax\n",
    ")\n",
    "\n",
    "co2_piv_subset.plot(\n",
    "    figsize=(15,8),\n",
    "    xticks=gl_piv_subset.index,\n",
    "    ylabel='Gross Load MW',\n",
    "    xlabel='Months',\n",
    "    ax=co2_ax\n",
    ")\n",
    "\n",
    "gl_ax.set_title('CEMS State-Level Gross Load 2018',fontsize= 18, pad=20)\n",
    "co2_ax.set_title('CEMS State-Level CO2 Emissions 2018', fontsize=18, pad=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot CO2 to gross load comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add field for comparison\n",
    "my_cems_gl['co2/load'] = my_cems_gl.co2_mass_tons / my_cems_gl.gross_load_mw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table around comparison field\n",
    "gl_co2_piv = my_cems_gl.pivot(columns='state', index=['month'], values=['co2/load'])\n",
    "gl_co2_piv_subset = gl_co2_piv.iloc[:, gl_co2_piv.columns.get_level_values(1).isin(state_subset)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure to plot the different values\n",
    "fig, gl_co2_ax = plt.subplots()\n",
    "\n",
    "gl_co2_piv_subset.plot(\n",
    "    figsize=(15,8),\n",
    "    xticks=gl_co2_piv_subset.index,\n",
    "    xlabel='Months',\n",
    "    ylabel='CO2 Emissions (Tons)',\n",
    "    ax=gl_co2_ax\n",
    ")\n",
    "\n",
    "gl_co2_ax.set_title('State CO2 Emissions / Gross Load in 2018',fontsize= 18, pad=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
