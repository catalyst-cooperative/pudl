{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with EPA CEMS data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CEMS or <a href='https://www.epa.gov/emc/emc-continuous-emission-monitoring-systems'>**Continuous Emissions Monitoring Systems**</a> are used to track power plant's compliance with EPA emission standards. Included the data are hourly measurements of gross load, SO2, CO2, and NOx emissions associated with a given point source. The EPA's <a href='https://www.epa.gov/airmarkets'>Clean Air Markets Division</a> has collected CEMS data stretching back to 1995 and publicized it in their <a href='https://campd.epa.gov/'>data portal</a>. Combinging the CEMS data with geospatial, EIA and FERC data can enable greater and more specific analysis of utilities and their generation facilities. This notebook provides examples of working with the CEMS data in pudl."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook Contents:\n",
    "* **<a href='#setup'>Setup</a>**\n",
    "* **<a href='#access'>Accessing CEMS data</a>**\n",
    " - <a href='#1subset'>1. Select a subset of raw data using Dask</a>\n",
    " - <a href='#2transfer'>2. Transfer desired data to pandas</a>\n",
    " - <a href='#3pickle'>3. Export custom CEMS dataframe as CSV</a>\n",
    "* **<a href='#manipulating'>Manipulating & Visualizing CEMS data</a>**\n",
    " - <a href='#emap'>1. Simple Choropleth</a>\n",
    " - <a href='#pcem'>2. Proportional Coordinates Map</a>\n",
    " - <a href='#glc'>3. State-to-State Gross Load Comparison</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "# import logging\n",
    "# import sys\n",
    "import os\n",
    "# import pathlib\n",
    "\n",
    "# 3rd party libraries\n",
    "import geopandas as gpd\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import sqlalchemy as sa\n",
    "\n",
    "# Local libraries\n",
    "#import pudl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Enable viewing of logging outputs\n",
    "# logger=logging.getLogger()\n",
    "# logger.setLevel(logging.INFO)\n",
    "# handler = logging.StreamHandler(stream=sys.stdout)\n",
    "# formatter = logging.Formatter('%(message)s')\n",
    "# handler.setFormatter(formatter)\n",
    "# logger.handlers = [handler]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following settings and variables may be changed to impact the processing of this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display settings\n",
    "# sns.set()\n",
    "# %matplotlib inline\n",
    "# mpl.rcParams['figure.dpi'] = 150\n",
    "# pd.options.display.max_columns = 100\n",
    "# pd.options.display.max_rows = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CEMS dates\n",
    "# CEMS_year = 2018\n",
    "# CEMS_year_range = range(2010, 2012)\n",
    "\n",
    "# # State selection\n",
    "# state_subset = ['CO', 'TX', 'WY', 'MN', 'OH', 'PA', 'WV', 'FL', 'GE', 'CA']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='access'></a>\n",
    "## Accessing CEMS data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CEMS dataset is enormous! It contains hourly emissions data on an hourly basis going back to 1995 meaning that the full dataset is close to a billion rows and takes up 100 GB of space. That's a lot to process when you may only need a fraction of it for analysis. The following steps will help you access and work with CEMS efficiently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1access'></a>\n",
    "### 1. Access the full remote CEMS parquet using Dask\n",
    "\n",
    "Dask is a python package that parallelizes pandas dataframes so that you can access larger-than-memory data. With Dask, you can select the subset of CEMS data that you'd like to analyse *before* loading the data into a dataframe. While in Dask, you can interact with the data as if it were in a pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = \"s3://pudl.catalyst.coop/nightly\"\n",
    "table_name = \"core_epacems__hourly_emissions\"\n",
    "\n",
    "cems_full = dd.read_parquet(\n",
    "    f\"{parquet_path}/{table_name}.parquet\",\n",
    "    storage_options={\"anon\": True},\n",
    "    engine=\"pyarrow\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a Dask dataframe you can learn things about the data such as column names and datatypes without having to load all of the data. If you take a look at the length of the Dask dataframe, you'll understand why we're not in pandas yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cems_full) # This shows how many rows there are!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the CEMS fields available below. The records are organized by the date and time of their measurement as well as the EIA plant id (`plant_id_eia`) and EPA unit id (`emissions_unit_id_epa`) they coorespond to. \n",
    "\n",
    "The EPA unit id is EPA's most granular level of emissions tracing. It represents a singular \"smokestack\" where emissions data are monitored and recorded. Depending on the unit in question, this unit may reflect a single generator (in the case of a combustion gas turbine where emissions are directly associated with generation), or a group of inter-operating biolers and generators (such as with steam powered generators where one or more boilers, possibly with differing fuel types, provide mechanical power to turbines). As a result, the emissions unit id *does not map directly onto EIA's generator id*, rather, it serves as its own unique grouping. The EPA publishes a crosswalk that links EPA units to EIA's more granular boiler and generator ids. We'll get to that lower down!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cems_full.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cems_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2subset'></a>\n",
    "### 2. Subset the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know what's available, pick the columns you'd like to work with, and aggregate rows as necessary. Note that the `state` column is a categorical datatypes, meaning that they will overwhelm your computer memory if included in the list of columns you'd like to `groupby`. \n",
    "\n",
    "In pandas, this is solved by including the statement `observed=True` in the `groupby`, but with Dask we'll solve this by changing the datatype to string. As mentioned previously, the dataset is very large. If the Dask dataframe you construct is too similar to the original dataset -- imagine the example below without the `groupby` -- the client will be unable to load it in pandas and the kernel will attempt to run indefinitely (or until it crashes your computer). The dataset below should load in a couple of minutes when transfered to pandas.\n",
    "\n",
    "Do as much subsetting as you can in Dask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The years you'd like to work with\n",
    "years = [2024]\n",
    "\n",
    "# Select emissions data are grouped by state, plant_id and unit_id\n",
    "# Remember to change the datatype for 'state' from category to string\n",
    "cems_subset = (\n",
    "    cems_full.loc[cems_full[\"year\"].isin(years)]\n",
    "    .assign(state=lambda x: x['state'].astype('string'))\n",
    "    .groupby([\"year\", \"state\", \"plant_id_eia\",])[\n",
    "        ['so2_mass_lbs', 'nox_mass_lbs', 'co2_mass_tons']]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3transfer'></a>\n",
    "### 3. Transfer CEMS subset to pandas\n",
    "\n",
    "Now that you've selected the data you want to work with, we'll transfer it to pandas so that all rows are accessible. It'll take a minute or two to run because there are so many rows. If it takes a really long time, consider how you can subset the data further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialize the Dask computations\n",
    "cems_subset_df = cems_subset.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4export'></a>\n",
    "### 4. Export CEMS subset to CSV\n",
    "\n",
    "Because CEMS takes a while to run, it may be in your best interest to save your finalized CEMS dataframes as pickle files. This will prevent you from having to run the entire Dask-to-pandas process over and over if you restart the notebook and you want to access your carve-out of CEMS. Rather, it will save a local copy of your CEMS dataframe that it can access in a matter of seconds. Uncomment the following to set up a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export CEMS to CSV in current directory\n",
    "path = os.getcwd()\n",
    "cems_subset_df.to_pickle(path + '/cems_subset_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CEMS CSV\n",
    "my_cems_df = pd.read_pickle(path + '/cems_subset_df.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='manipulating'></a>\n",
    "## Manipulating & Visualizing CEMS data\n",
    "\n",
    "\n",
    "<span style=\"color:red\">[WARNING: WIP, the following code is antiquated and needs to be reworked!]</span>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have access to CEMS in pandas, lets see what we can do!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='emap'></a>\n",
    "### 1. Simple Choropleth\n",
    "##### *Visualizing CEMS data*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by mapping which states have the highest CO2 emissions from power plants in 2018. States with darker colors will indicate higher CO2 emissions. To do this, we'll need to merge a geodataframe of the US with the desired emissions data from each state."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prep US geospatial data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_census = pd.read_parquet(f\"{parquet_path}/out_censusdp1tract__states.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pre-existing pudl shapefile for state outlines\n",
    "# FIXME(zane): this is outdated and references non-existing method get_census2010_dgf\n",
    "pudl_settings = None\n",
    "assert pudl_settings is not None\n",
    "us_map_df = (\n",
    "    pudl.analysis.service_territory.get_census2010_gdf(pudl_settings, 'state')\n",
    "    .rename({'STUSPS10': 'state'}, axis=1)\n",
    "    .to_crs(\"EPSG:3395\") # Change the projection\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prep CEMS data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lbs to tons for so2 and nox and remove old columns\n",
    "# Aggregate CEMS emissions data to the state level\n",
    "cems_map_df = (\n",
    "    cems_subset_df.assign(\n",
    "        so2_mass_tons=lambda x: x.so2_mass_lbs * 0.0005,\n",
    "        nox_mass_tons=lambda x: x.nox_mass_lbs * 0.0005\n",
    "    ).drop(columns=['so2_mass_lbs', 'nox_mass_lbs', 'plant_id_eia'], axis=1)\n",
    "    .groupby(['state', 'year']).sum(min_count=1)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine with Geo-data and Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely import wkb\n",
    "\n",
    "# Combine CEMS and map dataframes\n",
    "states_cems_gdf = pd.merge(state_census, cems_map_df, on='state', how='outer')\n",
    "\n",
    "states_cems_gdf = gpd.GeoDataFrame(states_cems_gdf)\n",
    "\n",
    "# if your geometries are bytes:\n",
    "states_cems_gdf[\"geometry\"] = states_cems_gdf[\"geometry\"].apply(wkb.loads)\n",
    "\n",
    "# now tell GeoPandas which column is the geometry\n",
    "states_cems_gdf = states_cems_gdf.set_geometry(\"geometry\")\n",
    "\n",
    "states_cems_gdf = states_cems_gdf.set_crs(\"EPSG:4326\")\n",
    "\n",
    "states_cems_gdf[\"co2_mass_tons\"] = pd.to_numeric(\n",
    "    states_cems_gdf[\"co2_mass_tons\"], errors=\"coerce\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import geopandas as gpd\n",
    "\n",
    "# Make sure your data is in EPSG:4326 (lat/lon)\n",
    "states_cems_gdf = states_cems_gdf.to_crs(epsg=4326)\n",
    "\n",
    "# Create the base map (centered on the continental US)\n",
    "m = folium.Map(location=[37.8, -96], zoom_start=4, tiles=\"cartodbpositron\")\n",
    "\n",
    "# Add the choropleth layer\n",
    "folium.Choropleth(\n",
    "    geo_data=states_cems_gdf,\n",
    "    data=states_cems_gdf,\n",
    "    columns=[\"state\", \"co2_mass_tons\"],   # first is key, second is value\n",
    "    key_on=\"feature.properties.state\",\n",
    "    fill_color=\"YlOrRd\",\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name=\"CO₂ emissions (tons)\",\n",
    ").add_to(m)\n",
    "\n",
    "# Optional: add tooltips/popups with extra info\n",
    "folium.GeoJson(\n",
    "    states_cems_gdf,\n",
    "    name=\"States\",\n",
    "    tooltip=folium.features.GeoJsonTooltip(\n",
    "        fields=[\"state\", \"co2_mass_tons\"],\n",
    "        aliases=[\"State:\", \"CO₂ (tons):\"],\n",
    "        localize=True,\n",
    "    ),\n",
    ").add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Add plots for the US, HI, and AK\n",
    "\n",
    "# The column on which to base the choroplath\n",
    "choro_col = 'co2_mass_tons'\n",
    "\n",
    "us_fig, us_ax = plt.subplots(figsize=(15, 10))\n",
    "#ak_hi_fig, (ak_ax, hi_ax) = plt.subplots(ncols=2)\n",
    "\n",
    "states_cems_gdf.plot(column=choro_col, cmap='Reds', linewidth=0.8, ax=us_ax)\n",
    "#states_cems_gdf.plot(column=choro_col, cmap='Reds', linewidth=0.8, edgecolor='black', ax=ak_ax)\n",
    "#states_cems_gdf.plot(column=choro_col, cmap='Reds', linewidth=0.8, edgecolor='black', ax=hi_ax)\n",
    "\n",
    "#us_ax.set_xlim(-1.45e7, -0.7e7) # Used to position US in center of the graph\n",
    "#us_ax.set_ylim(0.25e7, 0.65e7)  # Used to position US in center of the graph\n",
    "us_ax.set_title('CO2 Emissions from Power Plants in 2018 (Megatons)', fontdict={'fontsize': '18'})\n",
    "us_ax.axis('off')  # Remove lat and long tick marks\n",
    "#ak_ax.set_xlim(1.9e7, 6.7e6)  #(-2e7, -1.4e7)\n",
    "#ak_ax.set_ylim(0.6e7, 1.2e7)\n",
    "#hi_ax.set_xlim(-1.71e7, -1.8e7)\n",
    "#hi_ax.set_ylim(2e6, 2.6e6)\n",
    "\n",
    "# Add a legend\n",
    "vmax = states_cems_gdf[f'{choro_col}'].max() / 1000000 # (convert from tons to megatons)\n",
    "sm = plt.cm.ScalarMappable(cmap='Reds', norm=plt.Normalize(vmin=0, vmax=vmax))\n",
    "sm._A = []\n",
    "#cbar = us_fig.colorbar(sm, orientation=\"horizontal\", pad=0, aspect = 50, label='CO2 Emissions (MT)')\n",
    "\n",
    "from matplotlib import axes\n",
    "axes.Axes.mouseover\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pcem'></a>\n",
    "### 2. Proportional Coordinates Map\n",
    "##### *Integrate CEMS emission quantities with EIA plant location data*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to integreate CEMS with other datasets in pudl, you'll need to start by integrating CEMS with a dataset that also has a field for `plant_id_eia`. If you want to integrate with FERC later on, you'll also want a dataset that has a field for `plant_id_pudl`. Integrating CEMS with EIA860 data will provide coordinates for each plant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prep CEMS data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate CEMS data to the plant level, adjust units for visualization purposes\n",
    "cems_df = (\n",
    "    my_cems_df\n",
    "    .copy()\n",
    "    .assign(\n",
    "        co2_mass_mt=lambda df: df.co2_mass_tons / 10000 # measure in 10K tons\n",
    "    ).drop(columns=['co2_mass_tons'], axis=1)\n",
    "    .groupby(['plant_id_eia', 'state', 'year'])\n",
    "    .sum(min_count=1)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prep EIA data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab EIA 860 plant data that matched the year selected for CEMS\n",
    "plants_eia860 = (\n",
    "    pudl_out.plants_eia860()\n",
    "    .assign(year=lambda df: df.report_date.dt.year)\n",
    "    .query(\"year==@CEMS_year\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine EIA and CEMS data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine CEMS and EIA on plant_id_eia, state, and year\n",
    "eia860_cems_df = (\n",
    "    pd.merge(plants_eia860, cems_df, on=['plant_id_eia', 'state', 'year'], how='inner')\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overlay Coordinates on Base Map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lat and long data cols into plotable points in geopandas\n",
    "# Make CRS compatile with base map\n",
    "eia860_cems_gdf = (\n",
    "    gpd.GeoDataFrame(\n",
    "        eia860_cems_df, geometry=gpd.points_from_xy(\n",
    "            eia860_cems_df.longitude, eia860_cems_df.latitude))\n",
    "    .set_crs(epsg=4326, inplace=True) # necessary step before to_crs(epsg=3395)\n",
    "    .to_crs(epsg=3395)\n",
    ")\n",
    "\n",
    "# Make a base map\n",
    "us_fig, us_ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "base = us_map_df.plot(color='white', edgecolor='black', ax=us_ax)\n",
    "\n",
    "us_ax.set_xlim(-1.45e7, -0.7e7) # Used to position US in center of the graph\n",
    "us_ax.set_ylim(0.25e7, 0.65e7)  # Used to position US in center of the graph\n",
    "us_ax.set_title('CO2 Emissions from Power Plants in 2018 (10K tons)', fontdict={'fontsize': '20'})\n",
    "us_ax.axis('off')  # Remove lat and long tick marks\n",
    "\n",
    "# Plot the coordinates on top of the base map\n",
    "eia860_cems_df['alpha_co2'] = eia860_cems_df['co2_mass_mt'] \n",
    "eia860_cems_gdf.plot(ax=base, marker='o', color='red', markersize=eia860_cems_df['co2_mass_mt'], alpha=0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='glc'></a>\n",
    "### 3. State-to-State Gross Load and Emissions Comparison\n",
    "##### *Compare state load and emissions profiles*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all states use locally sourced electricity, however, looking at load profiles of plants in a given state can provide a glimpse of who is responsible for the greatest fossil peak load. This allocation can also be done by utility, but requires a table that maps plant or unit ownership percentage by utility."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prep CEMS data (you'll have to re-load from Dask for this new data arrangement):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2018\n",
    "\n",
    "# A list of the columns you'd like to include in your analysis\n",
    "my_cols = [\n",
    "    'state',\n",
    "    'plant_id_eia', \n",
    "    'unitid',\n",
    "    'operating_datetime_utc',\n",
    "    'co2_mass_tons',\n",
    "    'gross_load_mw',\n",
    "]\n",
    "\n",
    "my_cems_dd = (\n",
    "    dd.read_parquet(epacems_path, columns=my_cols)\n",
    "    .assign(\n",
    "        state=lambda x: x['state'].astype('string'),\n",
    "        month=lambda x: x['operating_datetime_utc'].dt.month)\n",
    "    .groupby(['state', 'month'])['gross_load_mw', 'co2_mass_tons'].sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Create a pandas dataframe out of your Dask dataframe and add a column to \n",
    "# indicate the year the data are coming from.\n",
    "client = Client()\n",
    "my_cems_gl = (\n",
    "    client.compute(my_cems_dd)\n",
    "    .result()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Select state subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_piv = my_cems_gl.pivot(columns='state', index=['month'], values=['gross_load_mw'])\n",
    "gl_piv_subset = gl_piv.iloc[:, gl_piv.columns.get_level_values(1).isin(state_subset)].copy()\n",
    "\n",
    "co2_piv = my_cems_gl.pivot(columns='state', index=['month'], values=['co2_mass_tons'])\n",
    "co2_piv_subset = co2_piv.iloc[:, co2_piv.columns.get_level_values(1).isin(state_subset)].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot load and emissions comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (gl_ax, co2_ax) = plt.subplots(1,2)\n",
    "\n",
    "gl_piv_subset.plot(\n",
    "    figsize=(15,8),\n",
    "    xticks=gl_piv_subset.index,\n",
    "    ylabel='Gross Load MW',\n",
    "    xlabel='Months',\n",
    "    ax=gl_ax\n",
    ")\n",
    "\n",
    "co2_piv_subset.plot(\n",
    "    figsize=(15,8),\n",
    "    xticks=gl_piv_subset.index,\n",
    "    ylabel='Gross Load MW',\n",
    "    xlabel='Months',\n",
    "    ax=co2_ax\n",
    ")\n",
    "\n",
    "gl_ax.set_title('CEMS State-Level Gross Load 2018',fontsize= 18, pad=20)\n",
    "co2_ax.set_title('CEMS State-Level CO2 Emissions 2018', fontsize=18, pad=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot CO2 to gross load comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add field for comparison\n",
    "my_cems_gl['co2/load'] = my_cems_gl.co2_mass_tons / my_cems_gl.gross_load_mw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table around comparison field\n",
    "gl_co2_piv = my_cems_gl.pivot(columns='state', index=['month'], values=['co2/load'])\n",
    "gl_co2_piv_subset = gl_co2_piv.iloc[:, gl_co2_piv.columns.get_level_values(1).isin(state_subset)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure to plot the different values\n",
    "fig, gl_co2_ax = plt.subplots()\n",
    "\n",
    "gl_co2_piv_subset.plot(\n",
    "    figsize=(15,8),\n",
    "    xticks=gl_co2_piv_subset.index,\n",
    "    xlabel='Months',\n",
    "    ylabel='CO2 Emissions (Tons)',\n",
    "    ax=gl_co2_ax\n",
    ")\n",
    "\n",
    "gl_co2_ax.set_title('State CO2 Emissions / Gross Load in 2018',fontsize= 18, pad=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
