{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa12c4de-3887-4ff4-98a1-67c405db006b",
   "metadata": {},
   "source": [
    "# Using the VCE Resource Adequacy Renewable Energy (RARE) Power Dataset Parquet Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d8c7fe-8e6d-44ae-b68f-7d659c45f23d",
   "metadata": {},
   "source": [
    "The RARE dataset, `vcerare` in PUDL, was produced by Vibrant Clean Energy and is licensed to the public under the Creative Commons Attribution 4.0 International license (CC-BY-4.0). The data consists of hourly, county-level renewable generation profiles for solar pv, onshore, and offshore wind in the continental United States. It was compiled based on outputs from the NOAA HRRR weather model. Visit our [VCE Data Source](https://catalystcoop-pudl.readthedocs.io/en/nightly/data_sources/vcerare.html) page to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab75607-14f1-4407-8f82-f6c11c935f36",
   "metadata": {},
   "source": [
    "## There are two primary access methods for RARE data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95781d44-11f9-4d1f-9d2b-c1e6b3d80f11",
   "metadata": {},
   "source": [
    "* **Raw CSV archives (Zenodo)**\n",
    "* **Processed PUDL Parquet files (S3 bucket)**\n",
    "  \n",
    "\n",
    "If you saw CSV and said: *\"that's for me!\"* -- we're here to say: *\"give Parquet a chance!\"* This notebook is intended to help you navigate and customize this enormous dataset in a way that may actually make your life easier than trying to wrangle it in Excel.\n",
    "\n",
    "### Raw CSV archives (Zenodo)\n",
    "\n",
    "The raw data are separated into CSVs by year (2019 - 2023) and generation source (solar PV, onshore wind, offshore wind). Each file contains an index column for the hour of year (1 - 8760) and value columns containing estimated capacity factor for the county. The files have been optimized for distribution in Excel as they contain as little additional information as possible.\n",
    "\n",
    "See the [Zenodo archive README](https://zenodo.org/records/13937523) for more detail on the raw data schema and contents. \n",
    "\n",
    "### Processed PUDL Parquet files (S3 bucket)\n",
    "\n",
    "The processed data are published as Apache Parquet files--a file format designed for efficient data storage and retreival. These processed files combine all years and generation sources into one table. They also restructure the data from a wide format to a tall format by pulling county into a column value rather than a column header. Additional informational columns are added to the processed data including: \n",
    "\n",
    "* Latitude\n",
    "* Longitude\n",
    "* County FIPS code\n",
    "* Report rear\n",
    "* Datetime UTC\n",
    "* Separate state and county (or Lake) name fields (the raw data is formatted together as county_state)\n",
    "\n",
    "Take a look at our [data dictionary](https://catalystcoop-pudl.readthedocs.io/en/nightly/data_dictionaries/pudl_db.html#out-vcerare-hourly-available-capacity-factor) for more information on the processed table schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e861aa-01c8-4174-bac3-4e418b880f9a",
   "metadata": {},
   "source": [
    "## Which one should you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c509e-f518-4159-85f3-eb6764e980ca",
   "metadata": {},
   "source": [
    "If you want or need to alter the layout of the data in order to feed it into your model, are interested in a particular subset of the data, want to connect it to geospatial data, or want to view / run analysis on all the data at once, the processed PUDL Parquet files are the way to go! For context, the row limit in Excel is a little more than 1 million rows. The processed PUDL table contains 136,437,000 rows. Below, we'll show you how to wrangle this dataset without blowing up your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c399ea58-aa9b-45e1-9f0b-57adbc8d949e",
   "metadata": {},
   "source": [
    "## Accessing RARE Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef0416-3f43-48f2-b71a-76a383006f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import pudl\n",
    "pudl_paths = pudl.workspace.setup.PudlPaths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49394cc9-116b-4fee-a445-02afcc3215b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_df = pd.read_parquet(\n",
    "    pudl_paths.pudl_output / \"parquet/out_vceregen__hourly_available_capacity_factor.parquet\"\n",
    ")\n",
    "rare_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9553bd-3c69-4a70-b9e2-11dccd6cbfa5",
   "metadata": {},
   "source": [
    "### Filter the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87d1575-008c-4df9-a394-aaa3c875fde8",
   "metadata": {},
   "source": [
    "You can use this format to filter the data you'd like to access. Here are some examples: \n",
    "\n",
    "```\n",
    "# Filter by single year\n",
    "filtered_df = rare_df[rare_df[\"report_year\"]==2023]\n",
    "\n",
    "# Filter by multiple years\n",
    "filtered_df = rare_df[rare_df[\"report_year\"].isin([2019, 2020])]\n",
    "\n",
    "# Filter by state\n",
    "filtered_df = rare_df[rare_df[\"state\"]==\"TX\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d0cc7c-3b84-4dc8-b07d-4f2f6de8df79",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "filtered_df = rare_df[rare_df[\"state\"]==\"RI\"]\n",
    "filtered_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c6c8d-8a8e-48be-918d-6a9d1e14741f",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41d9ee-3f6b-4da6-8450-6f649bef2744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random county_id generator:\n",
    "random_county = random.choice(rare_df[\"county_id_fips\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e5f29-7c98-44a2-8578-b64a6878733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Editable fields: \n",
    "year = 2023\n",
    "county_id = random_county\n",
    "gen_type = \"onshore_wind\" # choose from: solar_pv, onshore_wind, offshore_wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915c2de-215f-4b25-b718-15f14ed3dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-editable variables\n",
    "plot_df = rare_df[\n",
    "    (rare_df[\"report_year\"]==year)\n",
    "    & (rare_df[\"county_id_fips\"]==county_id)\n",
    "]\n",
    "county_name = plot_df[plot_df[\"county_id_fips\"]==county_id].county_or_lake_name.unique().item()\n",
    "state_name = plot_df[plot_df[\"county_id_fips\"]==county_id].state.unique().item()\n",
    "\n",
    "# Make the chart\n",
    "plt.hist(plot_df[f\"capacity_factor_{gen_type}\"], bins=100, range=(0, 1))\n",
    "plt.title(\n",
    "    f\"county: {county_name}; state: {state_name}; gen: {gen_type}; max:{plot_df[f\"capacity_factor_{gen_type}\"].max():.2f}\")\n",
    "plt.grid()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d939e8-3a3d-4908-9ca3-67e4c69c35d8",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13289ced-0028-4d79-9e5a-cf79bc464d80",
   "metadata": {},
   "source": [
    "#### Memory check - will it Excel? \n",
    "\n",
    "If you want to know whether your table is capable of being processed as a csv you can use this memory estimator. If the estimated memory exceeds 500 MB it's too big! Cutting columns or making the filter scope smaller will help reduce the file size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47400458-83f7-4a6b-abea-55ae9618bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = the name of the table you want to test\n",
    "test_df = plot_df\n",
    "\n",
    "# Calculate the memory usage in bytes, including the index\n",
    "mem_usage_bytes = test_df.memory_usage(deep=True).sum()\n",
    "# Convert bytes to megabytes\n",
    "mem_usage_mb = mem_usage_bytes / (1024 * 1024)\n",
    "csv_size_estimate_mb = mem_usage_mb * 2  # Rough multiplier for CSV size\n",
    "print(f\"Estimated CSV size: {csv_size_estimate_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bff202-4766-44e1-8442-86108335162b",
   "metadata": {},
   "source": [
    "#### Download!\n",
    "Specify your desired download location by filling in the `download_path` and running this cell will output the data to that location under the name `rare_power_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c880907-818c-47e0-bfc9-2ee270d8893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the file path you want to download the data to\n",
    "download_path = \"\"\n",
    "plot_df.to_csv(download_path+\"/rare_power_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
