{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "import pudl\n",
    "import pudl.constants as pc\n",
    "from pudl.analysis.state_demand import (\n",
    "    load_ferc714_hourly_demand_matrix,\n",
    "    clean_ferc714_hourly_demand_matrix,\n",
    "    filter_ferc714_hourly_demand_matrix,\n",
    "    impute_ferc714_hourly_demand_matrix,\n",
    "    melt_ferc714_hourly_demand_matrix,\n",
    "    load_counties,\n",
    "    load_ferc714_county_assignments,\n",
    "    load_eia861_state_total_sales,\n",
    "    predict_state_hourly_demand,\n",
    "    plot_demand_timeseries,\n",
    "    plot_demand_scatter,\n",
    "    load_ventyx_hourly_state_demand,\n",
    "    lookup_state,\n",
    "    compare_state_demand,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger=logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(stream=sys.stdout)\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (10,4)\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pudl_in': '/home/zane/code/catalyst/pudl-work',\n",
       " 'data_dir': '/home/zane/code/catalyst/pudl-work/data',\n",
       " 'settings_dir': '/home/zane/code/catalyst/pudl-work/settings',\n",
       " 'pudl_out': '/home/zane/code/catalyst/pudl-work',\n",
       " 'sqlite_dir': '/home/zane/code/catalyst/pudl-work/sqlite',\n",
       " 'parquet_dir': '/home/zane/code/catalyst/pudl-work/parquet',\n",
       " 'datapkg_dir': '/home/zane/code/catalyst/pudl-work/datapkg',\n",
       " 'notebook_dir': '/home/zane/code/catalyst/pudl-work/notebook',\n",
       " 'ferc1_db': 'sqlite:////home/zane/code/catalyst/pudl-work/sqlite/ferc1.sqlite',\n",
       " 'pudl_db': 'sqlite:////home/zane/code/catalyst/pudl-work/sqlite/pudl.sqlite'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "API_KEY_EIA = os.environ[\"API_KEY_EIA\"]\n",
    "API_KEY_BLS = os.environ[\"API_KEY_BLS\"]\n",
    "API_KEY_FRED = os.environ[\"API_KEY_FRED\"]\n",
    "#HARVEST_TOKEN = os.environ[\"HARVEST_TOKEN\"]\n",
    "#HARVEST_ACCOUNT_ID = os.environ[\"HARVEST_ACCOUNT_ID\"]\n",
    "\n",
    "pudl_settings = pudl.workspace.setup.get_defaults()\n",
    "ferc1_engine = sa.create_engine(pudl_settings['ferc1_db'])\n",
    "pudl_engine = sa.create_engine(pudl_settings['pudl_db'])\n",
    "pudl_out = pudl.output.pudltabl.PudlTabl(pudl_engine=pudl_engine)\n",
    "pudl_settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the FERC 714 data\n",
    "* Should take ~1 minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the interim FERC 714 ETL process!\n",
      "Extracting respondent_id_ferc714 from CSV into pandas DataFrame.\n",
      "Extracting id_certification_ferc714 from CSV into pandas DataFrame.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zane/code/catalyst/pudl/src/pudl/extract/ferc714.py:59: UserWarning: Integration of FERC 714 into PUDL is still experimental and incomplete.\n",
      "The data has not yet been validated, and the structure may change.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting gen_plants_ba_ferc714 from CSV into pandas DataFrame.\n",
      "Extracting demand_monthly_ba_ferc714 from CSV into pandas DataFrame.\n",
      "Extracting net_energy_load_ba_ferc714 from CSV into pandas DataFrame.\n",
      "Extracting adjacency_ba_ferc714 from CSV into pandas DataFrame.\n",
      "Extracting interchange_ba_ferc714 from CSV into pandas DataFrame.\n",
      "Extracting lambda_hourly_ba_ferc714 from CSV into pandas DataFrame.\n",
      "Extracting lambda_description_ferc714 from CSV into pandas DataFrame.\n",
      "Extracting description_pa_ferc714 from CSV into pandas DataFrame.\n",
      "Extracting demand_forecast_pa_ferc714 from CSV into pandas DataFrame.\n",
      "Extracting demand_hourly_pa_ferc714 from CSV into pandas DataFrame.\n",
      "Transforming respondent_id_ferc714.\n",
      "Transforming id_certification_ferc714.\n",
      "Transforming gen_plants_ba_ferc714.\n",
      "Transforming demand_monthly_ba_ferc714.\n",
      "Transforming net_energy_load_ba_ferc714.\n",
      "Transforming adjacency_ba_ferc714.\n",
      "Transforming interchange_ba_ferc714.\n",
      "Transforming lambda_hourly_ba_ferc714.\n",
      "Transforming lambda_description_ferc714.\n",
      "Transforming description_pa_ferc714.\n",
      "Transforming demand_forecast_pa_ferc714.\n",
      "Transforming demand_hourly_pa_ferc714.\n",
      "CPU times: user 33.5 s, sys: 22.4 s, total: 55.9 s\n",
      "Wall time: 57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df1, tz = load_ferc714_hourly_demand_matrix(pudl_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the FERC 714 hourly demand matrix\n",
    "* This uses ~32GB of memory.\n",
    "* Takes ~10 minutes.\n",
    "* Why the `RuntimeWarning` about an All-NaN slice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zane/miniconda3/envs/pudl-dev/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:993: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanmedian1d, axis, a, overwrite_input)\n",
      "/home/zane/miniconda3/envs/pudl-dev/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1389: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanquantile_1d, axis, a, q,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 50s, sys: 2min 26s, total: 6min 17s\n",
      "Wall time: 7min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df2 = clean_ferc714_hourly_demand_matrix(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove respondents lacking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulled short respondent-years (below min_data):\n",
      "id\n",
      "201    [2006, 2019]\n",
      "Name: year, dtype: object\n",
      "Nulled bad respondent-years (below min_data_fraction):\n",
      "id\n",
      "115    [2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]                                    \n",
      "150    [2007]                                                                              \n",
      "161    [2008, 2009]                                                                        \n",
      "201    [2006]                                                                              \n",
      "260    [2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]\n",
      "Name: year, dtype: object\n",
      "Dropped blank respondents: [137, 148, 153, 154, 161, 201, 208, 260, 288, 293, 294]\n",
      "CPU times: user 1 s, sys: 103 ms, total: 1.11 s\n",
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df3 = filter_ferc714_hourly_demand_matrix(df2, min_data=100, min_data_fraction=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute missing demand values.\n",
    "* Very CPU intensive, takes ~1 hour and maxes out all 4 of my cores.\n",
    "* Not very memory intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing year 2006\n",
      "Iteration: 210\n",
      "Imputing year 2007\n",
      "Iteration: 201\n",
      "Imputing year 2008\n",
      "Iteration: 216\n",
      "Imputing year 2009\n",
      "Iteration: 206\n",
      "Imputing year 2010\n",
      "Iteration: 202\n",
      "Imputing year 2011\n",
      "Iteration: 213\n",
      "Imputing year 2012\n",
      "Iteration: 196\n",
      "Imputing year 2013\n",
      "Iteration: 210\n",
      "Imputing year 2014\n",
      "Iteration: 206\n",
      "Imputing year 2015\n",
      "Iteration: 193\n",
      "Imputing year 2016\n",
      "Iteration: 198\n",
      "Imputing year 2017\n",
      "Iteration: 201\n",
      "Imputing year 2018\n",
      "Iteration: 226\n",
      "Imputing year 2019\n",
      "Iteration: 201\n",
      "CPU times: user 34min 55s, sys: 15min 11s, total: 50min 7s\n",
      "Wall time: 16min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df4 = impute_ferc714_hourly_demand_matrix(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melt FERC 714 hourly demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.67 s, sys: 956 ms, total: 5.62 s\n",
      "Wall time: 5.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "demand = melt_ferc714_hourly_demand_matrix(df4, tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've already got the 2010 Census GeoDB.\n",
      "Extracting the GeoDB into a GeoDataFrame\n",
      "CPU times: user 2.77 s, sys: 229 ms, total: 3 s\n",
      "Wall time: 3.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counties = load_counties(pudl_out, pudl_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the interim EIA 861 ETL process!\n",
      "Extracting eia861 spreadsheet data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zane/code/catalyst/pudl/src/pudl/extract/eia861.py:35: UserWarning: Integration of EIA 861 into PUDL is still experimental and incomplete.\n",
      "The data has not yet been validated, and the structure may change.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "Transforming raw EIA 861 DataFrames for service_territory_eia861 concatenated across all years.\n",
      "Assigned state FIPS codes for 100.00% of records.\n",
      "Assigned county FIPS codes for 99.65% of records.\n",
      "Transforming raw EIA 861 DataFrames for balancing_authority_eia861 concatenated across all years.\n",
      "Started with 37622 missing BA Codes out of 39086 records (96.25%)\n",
      "Ended with 12674 missing BA Codes out of 39086 records (32.43%)\n",
      "Transforming raw EIA 861 DataFrames for sales_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Sales table.\n",
      "Dropped 0 duplicate records from EIA 861 Sales table, out of a total of 336550 records (0.0000% of all records). \n",
      "Performing value transformations on EIA 861 Sales table.\n",
      "Transforming raw EIA 861 DataFrames for advanced_metering_infrastructure_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Advanced Metering Infrastructure table.\n",
      "Transforming raw EIA 861 DataFrames for demand_response_eia861 concatenated across all years.\n",
      "Dropped 0 duplicate records from EIA 861 Demand Response Water Heater table, out of a total of 3105 records (0.0000% of all records). \n",
      "Tidying the EIA 861 Demand Response table.\n",
      "Dropped 0 duplicate records from EIA 861 Demand Response table, out of a total of 12452 records (0.0000% of all records). \n",
      "Performing value transformations on EIA 861 Demand Response table.\n",
      "Transforming raw EIA 861 DataFrames for demand_side_management_eia861 concatenated across all years.\n",
      "The following reported NERC regions are not currently recognized and become UNK values: []\n",
      "Dropped 0 duplicate records from EIA 861 Demand Side Management Misc. table, out of a total of 71640 records (0.0000% of all records). \n",
      "Transforming raw EIA 861 DataFrames for distributed_generation_eia861 concatenated across all years.\n",
      "Converting pct values into mw values for distributed generation misc table\n",
      "Converting pct values into mw values for distributed generation tech table\n",
      "Tidying Distributed Generation Tech Table\n",
      "Tidying Distributed Generation Fuel Table\n",
      "Transforming raw EIA 861 DataFrames for distribution_systems_eia861 concatenated across all years.\n",
      "Transforming raw EIA 861 DataFrames for dynamic_pricing_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Dynamic Pricing table.\n",
      "Performing value transformations on EIA 861 Dynamic Pricing table.\n",
      "Transforming raw EIA 861 DataFrames for energy_efficiency_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Energy Efficiency table.\n",
      "Transforming the EIA 861 Energy Efficiency table.\n",
      "Transforming raw EIA 861 DataFrames for green_pricing_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Green Pricing table.\n",
      "Performing value transformations on EIA 861 Green Pricing table.\n",
      "Transforming raw EIA 861 DataFrames for mergers_eia861 concatenated across all years.\n",
      "Transforming raw EIA 861 DataFrames for net_metering_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Net Metering table.\n",
      "Transforming raw EIA 861 DataFrames for non_net_metering_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Non Net Metering table.\n",
      "Transforming raw EIA 861 DataFrames for operational_data_eia861 concatenated across all years.\n",
      "The following reported NERC regions are not currently recognized and become UNK values: ['MPS', 'CLECO', 'MISE']\n",
      "Transforming raw EIA 861 DataFrames for reliability_eia861 concatenated across all years.\n",
      "Tidying the EIA 861 Reliability table.\n",
      "Dropped 0 duplicate records from EIA 861 Reliability table, out of a total of 14760 records (0.0000% of all records). \n",
      "Transforming raw EIA 861 DataFrames for utility_data_eia861 concatenated across all years.\n",
      "The following reported NERC regions are not currently recognized and become UNK values: ['MPS', 'SASKATCHWA', 'MISE']\n",
      "Tidying the EIA 861 Utility Data tables.\n",
      "Building an EIA 861 BA-Util-State association table.\n",
      "Building an EIA 861 Util-State-Date association table.\n",
      "Completing normalization of balancing_authority_eia861.\n",
      "CPU times: user 1min 58s, sys: 912 ms, total: 1min 59s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "assignments = load_ferc714_county_assignments(pudl_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.6 ms, sys: 688 µs, total: 50.2 ms\n",
      "Wall time: 49.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "state_totals = load_eia861_state_total_sales(pudl_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.79 s, sys: 2.13 s, total: 10.9 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prediction = predict_state_hourly_demand(\n",
    "    demand,\n",
    "    counties=counties,\n",
    "    assignments=assignments,\n",
    "    state_totals=state_totals,\n",
    "    mean_overlaps=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 489 µs, sys: 212 µs, total: 701 µs\n",
      "Wall time: 773 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pathlib\n",
    "local_dir = pathlib.Path(pudl_settings['data_dir']) / 'local'\n",
    "ventyx_path = local_dir / 'ventyx/state_level_load_2007_2018.csv'\n",
    "base_dir = local_dir / 'state-demand'\n",
    "base_dir.mkdir(parents=True, exist_ok=True)\n",
    "demand_path = base_dir / 'demand.csv'\n",
    "stats_path = base_dir / 'demand-stats.csv'\n",
    "timeseries_dir = base_dir / 'timeseries'\n",
    "timeseries_dir.mkdir(parents=True, exist_ok=True)\n",
    "scatter_dir = base_dir / 'scatter'\n",
    "scatter_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.2 s, sys: 267 ms, total: 58.5 s\n",
      "Wall time: 58.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Write predicted hourly state demand\n",
    "prediction.to_csv(\n",
    "    demand_path, index=False, date_format='%Y%m%dT%H', float_format='%.1f'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.3 s, sys: 4.49 s, total: 48.8 s\n",
      "Wall time: 52.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load Ventyx as reference if available\n",
    "reference = None\n",
    "if ventyx_path.exists():\n",
    "    reference = load_ventyx_hourly_state_demand(ventyx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 16s, sys: 2.54 s, total: 3min 18s\n",
      "Wall time: 3min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Plots and statistics\n",
    "stats = []\n",
    "for fips in prediction['state_id_fips'].unique():\n",
    "    state = lookup_state(fips)\n",
    "    # Filter demand by state\n",
    "    a = prediction.query(f'state_id_fips == {fips}')\n",
    "    b = None\n",
    "    title = f'{state[\"fips\"]}: {state[\"name\"]} ({state[\"code\"]})'\n",
    "    plot_name = f'{state[\"fips\"]}-{state[\"name\"]}.png'\n",
    "    if reference is not None:\n",
    "        b = reference.query(f'state_id_fips == {fips}')\n",
    "    # Save timeseries plot\n",
    "    plot_demand_timeseries(\n",
    "        a, b=b, window=168, title=title, path=timeseries_dir / plot_name\n",
    "    )\n",
    "    if b is None or b.empty:\n",
    "        continue\n",
    "    # Align predicted and reference demand\n",
    "    a = a.set_index('utc_datetime')\n",
    "    b = b.set_index('utc_datetime')\n",
    "    index = a.index.intersection(b.index)\n",
    "    a = a.loc[index].reset_index()\n",
    "    b = b.loc[index].reset_index()\n",
    "    # Compute statistics\n",
    "    stat = compare_state_demand(a, b, scaled=True)\n",
    "    stat['state_id_fips'] = fips\n",
    "    stats.append(stat)\n",
    "    # Save scatter plot\n",
    "    plot_demand_scatter(a, b=b, title=title, path=scatter_dir / plot_name)\n",
    "\n",
    "# Write statistics\n",
    "if reference is not None:\n",
    "    pd.concat(stats, ignore_index=True).to_csv(\n",
    "        stats_path, index=False, float_format='%.1f'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
