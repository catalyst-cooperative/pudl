{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dagster import AssetKey\n",
    "from pudl.etl import defs\n",
    "import pandas as pd\n",
    "from pudl.helpers import zero_pad_numeric_string, standardize_phone_column, standardize_na_values\n",
    "import numpy as np\n",
    "import re\n",
    "from pudl.metadata.dfs import POLITICAL_SUBDIVISIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = defs.load_asset_value(AssetKey(\"raw_phmsagas__yearly_distribution\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARLY_DISTRIBUTION_OPERATORS_COLUMNS = {\n",
    "    \"columns_to_keep\": [\n",
    "        \"report_date\",\n",
    "        \"report_number\",  # not in pudl/metadata/fields.py\n",
    "        \"report_submission_type\",  # not in pudl/metadata/fields.py\n",
    "        \"report_year\",\n",
    "        # None of the columns below are in pudl/metadata/fields.py\n",
    "        \"operator_id_phmsa\",\n",
    "        \"operator_name_phmsa\",\n",
    "        \"office_address_street\",\n",
    "        \"office_address_city\",\n",
    "        \"office_address_state\",\n",
    "        \"office_address_zip\",\n",
    "        \"office_address_county\",\n",
    "        \"headquarters_address_street\",\n",
    "        \"headquarters_address_city\",\n",
    "        \"headquarters_address_state\",\n",
    "        \"headquarters_address_zip\",\n",
    "        \"headquarters_address_county\",\n",
    "        \"excavation_damage_excavation_practices\",\n",
    "        \"excavation_damage_locating_practices\",\n",
    "        \"excavation_damage_one_call_notification\",\n",
    "        \"excavation_damage_other\",\n",
    "        \"excavation_damage_total\",\n",
    "        \"excavation_tickets\",\n",
    "        \"services_efv_in_system\",\n",
    "        \"services_efv_installed\",\n",
    "        \"services_shutoff_valve_in_system\",\n",
    "        \"services_shutoff_valve_installed\",\n",
    "        \"federal_land_leaks_repaired_or_scheduled\",\n",
    "        \"percent_unaccounted_for_gas\",\n",
    "        \"additional_information\",\n",
    "        \"preparer_email\",\n",
    "        \"preparer_fax\",\n",
    "        \"preparer_name\",\n",
    "        \"preparer_phone\",\n",
    "        \"preparer_title\",\n",
    "    ],\n",
    "    \"columns_to_convert_to_ints\": [\n",
    "        \"report_year\",\n",
    "        \"report_number\",\n",
    "        \"operator_id_phmsa\",\n",
    "        \"excavation_damage_excavation_practices\",\n",
    "        \"excavation_damage_locating_practices\",\n",
    "        \"excavation_damage_one_call_notification\",\n",
    "        \"excavation_damage_other\",\n",
    "        \"excavation_damage_total\",\n",
    "        \"excavation_tickets\",\n",
    "        \"services_efv_in_system\",\n",
    "        \"services_efv_installed\",\n",
    "        \"services_shutoff_valve_in_system\",\n",
    "        \"services_shutoff_valve_installed\",\n",
    "    ],\n",
    "    \"capitalization_exclusion\": [\"headquarters_address_state\", \"office_address_state\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df.loc[\n",
    "    :, YEARLY_DISTRIBUTION_OPERATORS_COLUMNS[\"columns_to_keep\"]\n",
    "].copy()\n",
    "\n",
    "# Standardize NAs\n",
    "df = standardize_na_values(df)\n",
    "\n",
    "# Initial string cleaning\n",
    "for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    df[col] = df[col].str.strip()\n",
    "\n",
    "# Specify the columns to convert to integer type\n",
    "cols_to_convert = YEARLY_DISTRIBUTION_OPERATORS_COLUMNS[\n",
    "    \"columns_to_convert_to_ints\"\n",
    "]\n",
    "\n",
    "# Fill NaN values with pd.NA, then cast to \"Int64\" nullable integer type\n",
    "df[cols_to_convert] = df[cols_to_convert].fillna(pd.NA).astype(\"Int64\")\n",
    "\n",
    "# Ensure all \"report_year\" values have four digits\n",
    "mask = df[\"report_year\"] < 100\n",
    "\n",
    "# Convert 2-digit years to appropriate 4-digit format (assume cutoff at year 50)\n",
    "# We could also use the first 4 digits of the \"report_number\" but there was at least one anomaly here with an invalid year\n",
    "df.loc[mask, \"report_year\"] = 2000 + df.loc[mask, \"report_year\"].where(\n",
    "    df.loc[mask, \"report_year\"] < 50, 1900\n",
    ")\n",
    "\n",
    "# Standardize case for city, county, operator name, etc.\n",
    "# Capitalize the first letter of each word in a list of columns\n",
    "cap_cols = df.select_dtypes(include=[\"object\"]).columns.difference(\n",
    "    YEARLY_DISTRIBUTION_OPERATORS_COLUMNS[\"capitalization_exclusion\"]\n",
    ")\n",
    "for col in cap_cols:\n",
    "    df[col] = df[col].str.title()\n",
    "\n",
    "# Standardize state abbreviations\n",
    "state_to_abbr = {\n",
    "    x.subdivision_name: x.subdivision_code\n",
    "    for x in POLITICAL_SUBDIVISIONS.itertuples()\n",
    "    if x.country_code == \"USA\" and x.subdivision_type == \"state\"\n",
    "}\n",
    "state_to_abbr.update(\n",
    "    {\n",
    "        x.subdivision_code: x.subdivision_code\n",
    "        for x in POLITICAL_SUBDIVISIONS.itertuples()\n",
    "        if x.country_code == \"USA\" and x.subdivision_type == \"state\"\n",
    "    }\n",
    ")\n",
    "\n",
    "for state_col in [\"headquarters_address_state\", \"office_address_state\"]:\n",
    "    df[state_col] = (\n",
    "        df[state_col]\n",
    "        .str.strip()\n",
    "        .replace(state_to_abbr)\n",
    "        .where(df[state_col].isin(state_to_abbr.values()), pd.NA)\n",
    "    )\n",
    "\n",
    "# Standardize zip codes\n",
    "df[\"office_address_zip\"] = zero_pad_numeric_string(\n",
    "    df[\"office_address_zip\"], n_digits=5\n",
    ")\n",
    "df[\"headquarters_address_zip\"] = zero_pad_numeric_string(\n",
    "    df[\"headquarters_address_zip\"], n_digits=5\n",
    ")\n",
    "\n",
    "# Standardize telephone and fax number format and drop (000)-000-0000\n",
    "df = standardize_phone_column(df, [\"preparer_phone\", \"preparer_fax\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.percent_unaccounted_for_gas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(negative_count / (positive_count + negative_count)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "positive_count = (df['percent_unaccounted_for_gas'] > 0).sum()\n",
    "negative_count = (df['percent_unaccounted_for_gas'] < 0).sum()\n",
    "\n",
    "# Data for plotting\n",
    "labels = ['Positive', 'Negative']\n",
    "counts = [positive_count, negative_count]\n",
    "\n",
    "# Create the bar plot\n",
    "plt.bar(labels, counts, color=['green', 'red'])\n",
    "plt.title('Distribution of Positive vs Negative Values')\n",
    "plt.xlabel('Value Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)  # Rotate labels if necessary\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.percent_unaccounted_for_gas<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## core_phmsagas__yearly_distribution_operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df[[\n",
    "    \"report_date\",\n",
    "    \"report_number\",\n",
    "    \"report_submission_type\",\n",
    "    \"report_year\",\n",
    "    \"operator_id_phmsa\",\n",
    "    \"operator_name_phmsa\",\n",
    "    \"office_address_street\",\n",
    "    \"office_address_city\",\n",
    "    \"office_address_state\",\n",
    "    \"office_address_zip\",\n",
    "    \"office_address_county\",\n",
    "    \"headquarters_address_street\",\n",
    "    \"headquarters_address_city\",\n",
    "    \"headquarters_address_state\",\n",
    "    \"headquarters_address_zip\",\n",
    "    \"headquarters_address_county\",\n",
    "    \"excavation_damage_excavation_practices\",\n",
    "    \"excavation_damage_locating_practices\",\n",
    "    \"excavation_damage_one_call_notification\",\n",
    "    \"excavation_damage_other\",\n",
    "    \"excavation_damage_total\",\n",
    "    \"excavation_tickets\",\n",
    "    \"services_efv_in_system\",\n",
    "    \"services_efv_installed\",\n",
    "    \"services_shutoff_valve_in_system\",\n",
    "    \"services_shutoff_valve_installed\",\n",
    "    \"federal_land_leaks_repaired_or_scheduled\",\n",
    "    \"percent_unaccounted_for_gas\",\n",
    "    \"additional_information\",\n",
    "    \"preparer_email\",\n",
    "    \"preparer_fax\",\n",
    "    \"preparer_name\",\n",
    "    \"preparer_phone\",\n",
    "    \"preparer_title\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to ints\n",
    "# Excluded \"federal_land_leaks_repaired_or_scheduled\" from this list since there were a couple rows with decimal values\n",
    "columns_to_convert = [\n",
    "    \"report_year\",\n",
    "    \"report_number\",\n",
    "    \"operator_id_phmsa\",\n",
    "    \"excavation_damage_excavation_practices\",\n",
    "    \"excavation_damage_locating_practices\",\n",
    "    \"excavation_damage_one_call_notification\",\n",
    "    \"excavation_damage_other\",\n",
    "    \"excavation_damage_total\",\n",
    "    \"excavation_tickets\",\n",
    "    \"services_efv_in_system\",\n",
    "    \"services_efv_installed\",\n",
    "    \"services_shutoff_valve_in_system\",\n",
    "    \"services_shutoff_valve_installed\"\n",
    "]\n",
    "df[columns_to_convert] = df[columns_to_convert].astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all \"report_year\" values have four digits\n",
    "mask = df[\"report_year\"] < 100\n",
    "\n",
    "# Convert 2-digit years to appropriate 4-digit format (assume cutoff at year 50)\n",
    "# We could also use the first 4 digits of the \"report_number\" but there was at least one anomaly here with an invalid year\n",
    "df.loc[mask, \"report_year\"] = df.loc[mask, \"report_year\"].apply(\n",
    "    lambda x: 2000 + x if x < 50 else 1900 + x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operator Table Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA values with zeroes because these columns are simply counts.\n",
    "# Note that \"excavation_damage...\" columns should sum up to the value in \"excavation_damage_total\". However, many rows\n",
    "# (on the scale of thousands) do not actually sum up to \"excavation_damage_total\".\n",
    "columns_to_fill = [\n",
    "    \"excavation_damage_excavation_practices\",\n",
    "    \"excavation_damage_locating_practices\",\n",
    "    \"excavation_damage_one_call_notification\",\n",
    "    \"excavation_damage_other\",\n",
    "    \"excavation_damage_total\",\n",
    "    \"excavation_tickets\",\n",
    "    \"services_efv_in_system\",\n",
    "    \"services_efv_installed\",\n",
    "    \"services_shutoff_valve_in_system\",\n",
    "    \"services_shutoff_valve_installed\",\n",
    "    \"federal_land_leaks_repaired_or_scheduled\"\n",
    "]\n",
    "df[columns_to_fill] = df[columns_to_fill].fillna(0)\n",
    "\n",
    "# Fill in bad strings\n",
    "df = standardize_na_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize case for city, county, operator name, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalize the first letter of each word in all object-type columns except the excluded ones\n",
    "exclude_columns = ['headquarters_address_state', 'office_address_state']\n",
    "df[df.select_dtypes(include=['object']).columns.difference(exclude_columns)] = \\\n",
    "    df[df.select_dtypes(include=['object']).columns.difference(exclude_columns)].apply(lambda col: col.str.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize state abbreviations\n",
    "# First create a dictionary of state names to abbreviations\n",
    "state_to_abbr = {\n",
    "    x.subdivision_name: x.subdivision_code\n",
    "    for x in POLITICAL_SUBDIVISIONS.itertuples()\n",
    "    if x.country_code == \"USA\" and x.subdivision_type == \"state\"\n",
    "}\n",
    "# Add abbreviations to the dictionary\n",
    "state_to_abbr.update({\n",
    "    x.subdivision_code: x.subdivision_code\n",
    "    for x in POLITICAL_SUBDIVISIONS.itertuples()\n",
    "    if x.country_code == \"USA\" and x.subdivision_type == \"state\"\n",
    "})\n",
    "\n",
    "def standardize_state(state):\n",
    "    if pd.isna(state):\n",
    "        return state\n",
    "    state = state.strip()\n",
    "    standardized_state = state_to_abbr.get(state, state)\n",
    "    if standardized_state not in state_to_abbr.values():\n",
    "        return np.nan\n",
    "    return standardized_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"headquarters_address_state\"] = df[\"headquarters_address_state\"].apply(standardize_state)\n",
    "df[\"office_address_state\"] = df[\"office_address_state\"].apply(standardize_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim all the object-type columns\n",
    "df[df.select_dtypes(include=['object']).columns] = df.select_dtypes(include=['object']).applymap(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize telephone and fax number format and drop (000)-000-0000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = standardize_phone_column(df, [\"preparer_phone\", \"preparer_fax\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize zip codes\n",
    "df[\"office_address_zip\"] = zero_pad_numeric_string(df[\"office_address_zip\"], n_digits=5)\n",
    "df[\"headquarters_address_zip\"] = zero_pad_numeric_string(df[\"headquarters_address_zip\"], n_digits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip whitespace from all object (string) columns\n",
    "df[df.select_dtypes(include=[\"object\"]).columns] = df.select_dtypes(\n",
    "    include=[\"object\"]\n",
    ").apply(lambda col: col.map(lambda x: x.strip() if isinstance(x, str) else x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code that can be used to analyze missing values in a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_missing_values(\n",
    "    df: pd.DataFrame, custom_missing_values: list[str] = None\n",
    ") -> list[str]:\n",
    "    \"\"\"Analyze columns of a DataFrame for missing or invalid values.\n",
    "\n",
    "    PLEASE NOTE: No calls to this method should be included in any final\n",
    "    transformation scripts. This is purely for analysis and does not perform\n",
    "    any data transformation or cleaning.\n",
    "\n",
    "    This function checks each column for missing or custom missing values\n",
    "    and logs a summary of the findings for string (object), numeric, and\n",
    "    datetime columns.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame to analyze.\n",
    "        custom_missing_values: Optional list of custom values to consider\n",
    "            as \"missing\" (e.g., empty strings, specific strings like \"NA\",\n",
    "            \"NULL\", etc.). If not provided, defaults to a standard set.\n",
    "\n",
    "    Returns:\n",
    "        exception_cols: List of names of columns that couldn't be analyzed\n",
    "            due to a caught exception.\n",
    "    \"\"\"\n",
    "    nan_cols = []\n",
    "    exception_cols = []\n",
    "\n",
    "    # Use a default set of custom missing values if none are provided\n",
    "    if custom_missing_values is None:\n",
    "        custom_missing_values = [\n",
    "            \"\",\n",
    "            \" \",\n",
    "            \"NA\",\n",
    "            \"N/A\",\n",
    "            \"NULL\",\n",
    "            \"-\",\n",
    "            \"None\",\n",
    "            \"NaN\",\n",
    "            \"?\",\n",
    "            \"*\",\n",
    "            \"#\",\n",
    "        ]\n",
    "\n",
    "    # Analyze columns for missing values\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            logger.info(f\"Analyzing column: {col}\")\n",
    "\n",
    "            # Get the column values\n",
    "            col_data = df[col]\n",
    "\n",
    "            # Check if the column is of string (object) type\n",
    "            if col_data.dtype == \"object\":\n",
    "                # Count rows where the value is NaN, None, empty string, or custom missing values\n",
    "                none_count = col_data.isna().sum()  # Count None (NaN)\n",
    "                empty_string_count = (\n",
    "                    col_data.str.strip() == \"\"\n",
    "                ).sum()  # Count empty strings\n",
    "                custom_missing_count = col_data.isin(\n",
    "                    custom_missing_values\n",
    "                ).sum()  # Count custom missing values\n",
    "\n",
    "                total_nan_count = none_count + empty_string_count + custom_missing_count\n",
    "\n",
    "                if total_nan_count > 0:\n",
    "                    nan_cols.append(col)\n",
    "\n",
    "                # Output counts\n",
    "                logger.info(f\"Column '{col}' is a string type.\")\n",
    "                if none_count > 0:\n",
    "                    logger.warning(f\"Rows with None values: {none_count}\")\n",
    "                    logger.warning(df[df[col].isna()].head())\n",
    "                if empty_string_count > 0:\n",
    "                    logger.warning(f\"Rows with empty strings: {empty_string_count}\")\n",
    "                    logger.warning(df[df[col].str.strip() == \"\"].head())\n",
    "                if custom_missing_count > 0:\n",
    "                    logger.warning(\n",
    "                        f\"Rows with custom missing values: {custom_missing_count}\"\n",
    "                    )\n",
    "                    logger.warning(df[df[col].isin(custom_missing_values)].head())\n",
    "                if (\n",
    "                    none_count == 0\n",
    "                    and empty_string_count == 0\n",
    "                    and custom_missing_count == 0\n",
    "                ):\n",
    "                    logger.info(\"Found nothing worth reporting here\")\n",
    "\n",
    "            # Check if the column is numeric (int or float)\n",
    "            elif pd.api.types.is_numeric_dtype(col_data):\n",
    "                # Count NA values in the column\n",
    "                na_count = col_data.isna().sum()\n",
    "                # Count custom missing values in numeric columns (if applicable)\n",
    "                custom_missing_numeric_count = col_data.isin(\n",
    "                    [0]\n",
    "                ).sum()  # Assuming 0 is considered a missing value\n",
    "\n",
    "                if na_count > 0 or custom_missing_numeric_count > 0:\n",
    "                    nan_cols.append(col)\n",
    "\n",
    "                # Handle the non-NA data for further analysis\n",
    "                col_data_cleaned = col_data.dropna()\n",
    "\n",
    "                if not col_data_cleaned.empty:\n",
    "                    # Calculate min and max\n",
    "                    min_val = col_data_cleaned.min()\n",
    "                    max_val = col_data_cleaned.max()\n",
    "\n",
    "                    if min_val < 0 or na_count > 0 or custom_missing_numeric_count > 0:\n",
    "                        logger.warning(f\"Min value: {min_val}\")\n",
    "                        logger.warning(f\"Max value: {max_val}\")\n",
    "                    if na_count > 0:\n",
    "                        logger.warning(f\"Rows with NA values: {na_count}\")\n",
    "                        logger.warning(df[df[col].isna()].head())\n",
    "                    if custom_missing_numeric_count > 0:\n",
    "                        logger.warning(\n",
    "                            f\"Custom missing values (e.g., 0): {custom_missing_numeric_count}\"\n",
    "                        )\n",
    "                        logger.warning(df[df[col].isin([0])].head())\n",
    "                    if (\n",
    "                        min_val > 0\n",
    "                        and na_count == 0\n",
    "                        and custom_missing_numeric_count == 0\n",
    "                    ):\n",
    "                        logger.info(\"Found nothing worth reporting here\")\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        f\"Column '{col}' is numeric but contains only NA values.\"\n",
    "                    )\n",
    "\n",
    "            # Check if the column is a datetime type\n",
    "            elif pd.api.types.is_datetime64_any_dtype(col_data):\n",
    "                # Count NA values in the datetime column\n",
    "                na_count = col_data.isna().sum()\n",
    "                # Assuming custom missing values might be present in string form before conversion\n",
    "                custom_missing_count = col_data.isin(custom_missing_values).sum()\n",
    "\n",
    "                if na_count > 0 or custom_missing_count > 0:\n",
    "                    nan_cols.append(col)\n",
    "\n",
    "                # Handle the non-NA data for further analysis\n",
    "                col_data_cleaned = col_data.dropna()\n",
    "\n",
    "                if not col_data_cleaned.empty:\n",
    "                    # Output min and max datetime values\n",
    "                    min_date = col_data_cleaned.min()\n",
    "                    max_date = col_data_cleaned.max()\n",
    "\n",
    "                    if na_count > 0 or custom_missing_count > 0:\n",
    "                        logger.warning(f\"Min date: {min_date}\")\n",
    "                        logger.warning(f\"Max date: {max_date}\")\n",
    "                        logger.warning(f\"Rows with NA values: {na_count}\")\n",
    "                        logger.warning(df[df[col].isna()].head())\n",
    "                        logger.warning(f\"Custom missing values: {custom_missing_count}\")\n",
    "                        logger.warning(df[df[col].isin(custom_missing_values)].head())\n",
    "                    if na_count == 0 and custom_missing_count == 0:\n",
    "                        logger.info(\"Found nothing worth reporting here\")\n",
    "                else:\n",
    "                    logger.warning(\n",
    "                        f\"Column '{col}' is datetime but contains only NA values.\"\n",
    "                    )\n",
    "\n",
    "            # If the column is of some other type, simply note the type\n",
    "            else:\n",
    "                logger.info(f\"Column '{col}' is of type {col_data.dtype}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            exception_cols.append(col)\n",
    "            logger.warning(f\"Caught exception for column {col}: {e}\\n\")\n",
    "            continue\n",
    "\n",
    "    logger.info(f\"Columns with NaNs or custom missing values: {nan_cols}\")\n",
    "    logger.info(f\"Columns with exceptions during processing: {exception_cols}\")\n",
    "\n",
    "    return exception_cols\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pudl-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
