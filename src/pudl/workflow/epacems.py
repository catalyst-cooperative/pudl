"""Implements pipeline for processing EPA CEMS dataset."""
import itertools
import os
from typing import Any

import pandas as pd
import prefect
import pyarrow
from prefect import task, unmapped
from pyarrow import parquet

import pudl
from pudl import constants as pc
from pudl import dfc
from pudl.convert import epacems_to_parquet
from pudl.dfc import DataFrameCollection
from pudl.extract.epacems import EpaCemsPartition
from pudl.load import csv
from pudl.workflow.dataset_pipeline import DatasetPipeline
from pudl.workflow.eia import EiaPipeline


def _validate_params_partition(etl_params_og, tables):
    # if there is a `partition` in the package settings..
    partition_dict = {}
    try:
        partition_dict = etl_params_og['partition']
        # it should be a dictionary with tables (keys) and partitions (values)
        # so for each table, grab the list of the corresponding partition.
        for table in tables:
            try:
                for part in partition_dict[table]:
                    if part not in etl_params_og.keys():
                        raise AssertionError('Partion not recognized')
            except KeyError:
                pass
    except KeyError:
        partition_dict['partition'] = None
    return(partition_dict)


def write_epacems_parquet_files(df: pd.DataFrame, table_name: str, partition: EpaCemsPartition):
    """Writes epacems dataframes to parquet files."""
    schema = epacems_to_parquet.create_cems_schema()
    df = pudl.helpers.convert_cols_dtypes(df, "epacems")
    df = csv.reindex_table(df, table_name)
    # TODO(rousik): this is a dirty hack, year column should simply be part of the
    # dataframe all along, however reindex_table complains about it.
    df["year"] = int(partition.year)
    df.year = df.year.astype(int)

    table = pyarrow.Table.from_pandas(
        df, preserve_index=False, schema=schema).cast(schema)
    # FIXME(rousik): cast(schema) applies schema for the second time.
    # This looks unnecessary but prevents crash when calling write_to_dataset with
    # pyarrow~=2.0.0 that fails on missing metadata.
    # This code has already been fixed in the new pyarrow codebase but this may not have
    # yet been released and we have not updated our dependency versions yet.
    # Here's the offending line in pyarrow that throws an exception:
    # https://github.com/apache/arrow/blob/478286658055bb91737394c2065b92a7e92fb0c1/python/pyarrow/pandas_compat.py#L1184
    #
    # This should be fixed in the newer pyarrow releases and could be removed
    # once we update our dependency.
    if prefect.context.pudl_upload_to_gcs:
        output_path = os.path.join(
            prefect.context.pudl_upload_to_gcs, "parquet", "epacems")
    else:
        output_path = os.path.join(
            prefect.context.pudl_settings["parquet_dir"], "epacems")
    parquet.write_to_dataset(
        table,
        root_path=output_path,
        filesystem=prefect.context.pudl_filesystem,
        partition_cols=['year', 'state'],
        compression='snappy')


@task(target="epacems-{partition}", task_run_name="epacems-{partition}")  # noqa: FS003
def epacems_process_partition(
        partition: EpaCemsPartition,
        plant_utc_offset: pd.DataFrame) -> DataFrameCollection:
    """Runs extract and transform phases for a given epacems partition."""
    logger = prefect.context.get("logger")
    logger.info(f'Processing epacems partition {partition}')
    table_name = f'hourly_emissions_epacems_{partition.year}_{partition.state.lower()}'
    df = pudl.extract.epacems.extract_epacems(partition)
    df = pudl.transform.epacems.transform_epacems(df, plant_utc_offset)
    write_epacems_parquet_files(df, table_name, partition)
    return DataFrameCollection()  # return empty DFC because everything is on disk


class EpaCemsPipeline(DatasetPipeline):
    """Runs epacems tasks."""

    DATASET = 'epacems'

    def __init__(self, *args: Any, eia_pipeline: EiaPipeline = None, **kwargs: Any):
        """Initializes epacems pipeline, hooks it to the existing eia pipeline.

        epacems depends on the plants_entity_eia table that is generated by the
        EiaPipeline. If epacems is run, it will pull this table from the existing
        eia pipeline.

        Args:
          eia_pipeline: instance of EiaPipeline that holds the plants_entity_eia
            table.
        """
        self.eia_pipeline = eia_pipeline
        super().__init__(*args, **kwargs)

    @classmethod
    def validate_params(cls, etl_params):
        """Validate and normalize epacems parameters."""
        epacems_dict = {
            "epacems_years": etl_params.get("epacems_years", []),
            "epacems_states": etl_params.get("epacems_states", []),
        }
        if epacems_dict["epacems_states"] and epacems_dict["epacems_states"][0].lower() == "all":
            epacems_dict["epacems_states"] = sorted(pc.cems_states.keys())

        # CEMS is ALWAYS going to be partitioned by year and state. This means we
        # are functinoally removing the option to not partition or partition
        # another way. Nonetheless, we are adding it in here because we still need
        # to know what the partitioning is like for the metadata generation
        # (it treats partitioned tables differently).
        epacems_dict['partition'] = {'hourly_emissions_epacems':
                                     ['epacems_years', 'epacems_states']}
        # this is maybe unnecessary because we are hardcoding the partitions, but
        # we are still going to validate that the partitioning is
        epacems_dict['partition'] = _validate_params_partition(
            epacems_dict, [pc.epacems_tables])
        if not epacems_dict['partition']:
            raise AssertionError(
                'No partition found for EPA CEMS.'
                'EPA CEMS requires either states or years as a partion'
            )

        if not epacems_dict['epacems_years'] or not epacems_dict['epacems_states']:
            return None
        else:
            return epacems_dict

    def build(self, params):
        """Add epacems tasks to the flow."""
        if not self.all_params_present(params, ['epacems_states', 'epacems_years']):
            return None
        with self.flow:
            plants = pudl.transform.epacems.load_plant_utc_offset(
                self.eia_pipeline.get_table('plants_entity_eia'))

            partitions = [
                EpaCemsPartition(year=y, state=s)
                for y, s in itertools.product(params["epacems_years"], params["epacems_states"])]

            epacems_dfc = epacems_process_partition.map(
                partitions,
                plant_utc_offset=unmapped(plants))
            df = dfc.merge_list(epacems_dfc)
            return df
